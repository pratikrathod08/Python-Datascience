{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2d3c6",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "Anomaly detection, also known as outlier detection, is a data mining technique used to identify patterns or data points that deviate significantly from the norm in a dataset. These deviant patterns or data points are considered anomalies because they do not conform to the expected behavior or distribution of the majority of the data. The purpose of anomaly detection is to flag and investigate these unusual instances, as they may represent errors, fraud, unusual events, or potential security breaches. Anomaly detection is applicable in various domains, such as cybersecurity, finance, healthcare, manufacturing, and more, where detecting rare and abnormal occurrences is essential for maintaining system integrity and safety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f3a2e",
   "metadata": {},
   "source": [
    "## Ans : 2\n",
    "\n",
    "Anomaly detection poses several challenges, including:\n",
    "\n",
    "Lack of labeled data: In many real-world scenarios, anomalies are rare, making it challenging to collect enough labeled data to train supervised models effectively.\n",
    "\n",
    "Imbalanced datasets: Since anomalies are usually a minority class, imbalanced datasets can lead to biased models that perform poorly in detecting anomalies.\n",
    "\n",
    "Novelty detection: Anomaly detection models should be able to identify novel and previously unseen anomalies that might differ significantly from known anomalies.\n",
    "\n",
    "Feature engineering: Choosing relevant features and representation of data is crucial for the effectiveness of anomaly detection algorithms.\n",
    "\n",
    "High-dimensional data: High-dimensional feature spaces can lead to the \"curse of dimensionality,\" making it harder to distinguish normal and anomalous data points.\n",
    "\n",
    "False positives and false negatives: Striking the right balance between detecting anomalies accurately while minimizing false alarms is a challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf12f8f",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    " The main difference between unsupervised and supervised anomaly detection lies in the availability of labeled training data:\n",
    "\n",
    "Unsupervised Anomaly Detection: In unsupervised anomaly detection, the algorithm works with unlabeled data, meaning it doesn't have access to examples of anomalies during training. The algorithm aims to learn the normal patterns in the data and identify deviations as anomalies without explicit knowledge of what anomalies look like.\n",
    "\n",
    "Supervised Anomaly Detection: In supervised anomaly detection, the algorithm is trained on a dataset that contains both normal and anomalous instances. The algorithm uses this labeled data to learn the patterns of both normal and anomalous behavior. During testing, it can then classify new instances as either normal or anomalous based on what it learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5adce80",
   "metadata": {},
   "source": [
    "## Ans : 4\n",
    "\n",
    " Anomaly detection algorithms can be broadly categorized into the following types:\n",
    "\n",
    "Statistical Methods: These methods assume that the data follows a certain statistical distribution (e.g., Gaussian) and identify anomalies based on deviations from this assumed distribution.\n",
    "\n",
    "Machine Learning Methods: These methods use various machine learning techniques, such as clustering, classification, or density estimation, to identify anomalies in the data.\n",
    "\n",
    "Distance-based Methods: These methods measure the distance or dissimilarity between data points and identify instances that are farthest from the rest as anomalies.\n",
    "\n",
    "Density-based Methods: These methods focus on identifying regions of low data density and label data points in these regions as anomalies.\n",
    "\n",
    "Model-based Methods: These methods create models of normal behavior and identify deviations from these models as anomalies.\n",
    "\n",
    "Time Series Anomaly Detection: These methods specifically deal with time series data and identify anomalies based on unusual patterns or trends over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c6637",
   "metadata": {},
   "source": [
    "## Ans : 5\n",
    "\n",
    " Distance-based anomaly detection methods make the following assumptions:\n",
    "\n",
    "Distance Metric: They assume the existence of a meaningful distance or similarity metric to measure the dissimilarity between data points.\n",
    "\n",
    "Normal Data Distribution: The majority of the data is assumed to follow a certain distribution, such as a normal distribution or any other well-defined distribution.\n",
    "\n",
    "Anomaly Separability: Anomalies are assumed to be more distant from the normal data instances in the feature space. In other words, anomalies are expected to have significantly different characteristics compared to normal data points.\n",
    "\n",
    "Global vs. Local Anomalies: These methods may make assumptions about whether anomalies are global (rare throughout the entire dataset) or local (rare within specific regions or clusters of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ff02b",
   "metadata": {},
   "source": [
    "## Ans : 6\n",
    "\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the local density of data points. Here's a high-level overview of the LOF algorithm's computation for a given data point:\n",
    "\n",
    "Calculate Local Reachability Density (LRD): For each data point, the LRD is calculated based on the average local density of its k-nearest neighbors (where k is a user-defined parameter). It measures how isolated the data point is compared to its neighbors.\n",
    "\n",
    "Calculate Local Outlier Factor (LOF): The LOF for each data point is then computed as the ratio of its LRD to the LRDs of its k-nearest neighbors. The LOF value for a data point quantifies how much more or less dense it is compared to its neighbors. An LOF value significantly greater than 1 indicates the data point is in a sparser region (potential anomaly), while a value much less than 1 suggests the point is in a denser region (typical data point).\n",
    "\n",
    "Anomaly Ranking: The data points are ranked based on their LOF scores, with higher LOF scores indicating higher likelihood of being anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cd60e",
   "metadata": {},
   "source": [
    "## Ans : 7\n",
    "\n",
    "The Isolation Forest algorithm has two main parameters:\n",
    "\n",
    "Number of Trees (n_estimators): This parameter determines the number of isolation trees to build. More trees generally lead to better performance but also increase computational overhead.\n",
    "\n",
    "Subsample Size (max_samples): This parameter controls the number of samples drawn from the dataset to construct each isolation tree. It typically defaults to \"auto,\" which means using a sub-sample size of the dataset size for each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0cef8",
   "metadata": {},
   "source": [
    "## Ans : 8 \n",
    "\n",
    "To calculate the anomaly score of a data point using K-Nearest Neighbors (KNN), you can use the concept of the local outlier factor (LOF). The LOF is calculated based on the local density of a data point relative to its k-nearest neighbors. In this case, the data point has 2 neighbors (k=2) within a radius of 0.5.\n",
    "\n",
    "The formula for the LOF is as follows:\n",
    "LOF = (Average LRD of k-nearest neighbors) / LRD of the data point\n",
    "\n",
    "However, the Local Reachability Density (LRD) depends on the k-distance of the data point, which is the distance to its k-th nearest neighbor. Without additional information about the exact distances to the 2 neighbors and their classes, it's not possible to provide a specific LOF or anomaly score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3458ac2",
   "metadata": {},
   "source": [
    "## Ans : 9\n",
    "\n",
    " In the Isolation Forest algorithm, the anomaly score for a data point is computed based on its average path length in the isolation trees. The intuition is that anomalies are easier to isolate and will have shorter average path lengths, while normal data points will require more splits and have longer average path lengths.\n",
    "\n",
    "The average path length of a data point in isolation trees is denoted by E(h(x)), where h(x) is the path length of the data point x in a single tree. The anomaly score s(x) for the data point x is calculated as follows:\n",
    "\n",
    "s(x) = 2^(-E(h(x))/c(n))\n",
    "\n",
    "where c(n) is a normalization factor estimated as 2 * (log(n-1) + 0.5772156649), and n is the number of data points in the dataset.\n",
    "\n",
    "Given that you have 100 trees and a dataset of 3000 data points, let's assume the average path length for the specific data point is 5.0. We can plug these values into the formula to calculate the anomaly score:\n",
    "\n",
    "n = 3000\n",
    "E(h(x)) = 5.0\n",
    "c(n) = 2 * (log(3000-1) + 0.5772156649) ≈ 9.019\n",
    "\n",
    "anomaly score = 2^(-5.0/9.019) ≈ 0.482\n",
    "\n",
    "So, the anomaly score for the data point with an average path length of 5.0 is approximately 0.482. Higher anomaly scores indicate a higher likelihood of being an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7452d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
