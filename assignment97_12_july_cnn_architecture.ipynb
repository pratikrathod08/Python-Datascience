{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfd6971",
   "metadata": {},
   "source": [
    "# TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47de9c4",
   "metadata": {},
   "source": [
    "## Ans : 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32290aa9",
   "metadata": {},
   "source": [
    "Pooling in CNNs, such as Max Pooling, serves to reduce spatial dimensions, making computations more manageable. It retains essential information while decreasing the model's sensitivity to variations. Pooling helps achieve translation invariance, allowing the network to recognize features regardless of their position in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ad020",
   "metadata": {},
   "source": [
    "## Ans : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc460451",
   "metadata": {},
   "source": [
    "**Max Pooling** and **Min Pooling** are both types of pooling operations commonly used in convolutional neural networks (CNNs) to down-sample feature maps, reducing spatial dimensions and computational complexity. However, they differ in how they select values from the pooling window.\n",
    "\n",
    "### Max Pooling:\n",
    "\n",
    "In Max Pooling, for each region of the input feature map (pooling window), the operation retains the maximum value within that region. The idea is to capture the most important feature present in that specific part of the image. Max Pooling is advantageous because it emphasizes dominant features, making the network more robust to small variations, noise, or distortions in the input data. It is the most common type of pooling used in CNNs and helps in achieving translation invariance.\n",
    "\n",
    "### Min Pooling:\n",
    "\n",
    "In Min Pooling, for each region of the input feature map, the operation retains the minimum value within that region. Min Pooling focuses on the least prominent features present in the region. While it's not as commonly used as Max Pooling, Min Pooling could be useful in specific scenarios where the network needs to identify the least intense or smallest feature present in the input data. However, it might make the network more sensitive to noise and small variations in the input.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Selection Criteria:**\n",
    "   - **Max Pooling:** Retains the maximum value in the pooling window.\n",
    "   - **Min Pooling:** Retains the minimum value in the pooling window.\n",
    "\n",
    "2. **Robustness:**\n",
    "   - **Max Pooling:** Emphasizes dominant features, making the network more robust to variations and distortions.\n",
    "   - **Min Pooling:** Focuses on the least intense features, potentially making the network sensitive to noise.\n",
    "\n",
    "3. **Common Usage:**\n",
    "   - **Max Pooling:** Widely used in CNN architectures for tasks like image classification, object detection, and feature extraction.\n",
    "   - **Min Pooling:** Less common in mainstream applications but might find use in specialized scenarios where the least intense features are crucial.\n",
    "\n",
    "In most cases, Max Pooling is preferred due to its effectiveness in capturing important features, especially in tasks where identifying the most dominant patterns is essential, such as in image recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c9399",
   "metadata": {},
   "source": [
    "## Ans : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610d4fc",
   "metadata": {},
   "source": [
    "**Padding in CNN (Convolutional Neural Networks)**\n",
    "\n",
    "In CNNs, padding refers to the process of adding extra pixels or border around the input images before performing convolution operations. Padding can be applied to the input image or feature maps. The primary reason for using padding is to preserve the spatial dimensions of the input volume across layers. Let's delve into its significance:\n",
    "\n",
    "### **1. Preserving Spatial Dimensions:**\n",
    "   - **Without Padding:** When you apply convolution operations without padding, the spatial dimensions of the feature maps tend to reduce with each layer. This reduction can result in the loss of valuable information at the borders of the image, especially in deeper layers.\n",
    "   - **With Padding:** Adding pixels around the input image or feature maps maintains the original spatial dimensions. It ensures that the convolutional layers can process the pixels at the image's edges and corners, capturing more comprehensive information.\n",
    "\n",
    "### **2. Centering Convolution:**\n",
    "   - **Padding helps in centering the convolution operation:** When the convolutional kernel slides over the image, padding ensures that the kernel's center is aligned with the pixels of the input. This alignment is crucial for understanding the spatial relationships and patterns in the image.\n",
    "\n",
    "### **3. Controlling the Size of Feature Maps:**\n",
    "   - **Pooling and convolution operations affect feature map size:** After convolution and pooling operations, the size of feature maps decreases. By controlling the amount of padding, you can influence the size of feature maps after these operations. This control is essential for designing networks and managing the spatial hierarchy of features.\n",
    "\n",
    "### **4. Mitigating the Vanishing Gradient Problem:**\n",
    "   - **Helps with deep networks:** In deep CNNs, especially when using activation functions like ReLU, information at the edges of the image can be critical. Padding ensures that gradients can flow back through the network during backpropagation, mitigating the vanishing gradient problem and enabling the training of deeper networks.\n",
    "\n",
    "### **5. Handling Various Input Sizes:**\n",
    "   - **Padding allows flexibility with input sizes:** Padding is particularly useful when dealing with images of different sizes. By padding smaller images to match the dimensions of larger ones, you can process them through the same CNN architecture without resizing, preserving the integrity of the data.\n",
    "\n",
    "### **6. Edge and Corner Information:**\n",
    "   - **Preserving edge and corner details:** Features at the edges and corners of objects in the image are essential for accurate recognition. Padding ensures that these features are adequately processed by the network, enhancing the model's ability to recognize objects in various positions and orientations within the image.\n",
    "\n",
    "In summary, padding in CNNs is a crucial technique that ensures the network can effectively learn and extract features from input images. It maintains the spatial dimensions, handles different input sizes, and preserves important edge and corner information, all of which are essential for the network's overall performance and accuracy in tasks like image recognition and object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a711a1",
   "metadata": {},
   "source": [
    "## Ans : 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49097ff",
   "metadata": {},
   "source": [
    "### **Zero-Padding vs. Valid-Padding in CNNs: Effects on Output Feature Map**\n",
    "\n",
    "In convolutional neural networks (CNNs), zero-padding and valid-padding are techniques used to handle the spatial dimensions of feature maps after applying convolutional operations. Here's a comparison of their effects on the output feature map:\n",
    "\n",
    "### **Zero-Padding:**\n",
    "\n",
    "**1. Definition:**\n",
    "   - **Zero-Padding:** Adds zero-valued pixels around the input image or feature map.\n",
    "\n",
    "**2. Effect on Output Feature Map:**\n",
    "   - **Preserves Spatial Dimensions:** Zero-padding preserves the spatial dimensions of the input feature map. When you apply convolution with zero-padding, the output feature map retains the same height and width as the input, assuming a stride of 1.\n",
    "\n",
    "   - **Centers Convolution:** Padding ensures that the convolutional kernel's center aligns with the pixels of the input. This centering is essential for maintaining spatial relationships during feature extraction.\n",
    "\n",
    "   - **Mitigates Information Loss:** By adding border pixels with zero values, zero-padding prevents the loss of valuable information at the edges and corners of the feature map.\n",
    "\n",
    "   - **Control Over Feature Map Size:** You have control over the size of the output feature map. By adjusting the amount of padding, you can influence the dimensions of the output feature map after convolution and pooling operations.\n",
    "\n",
    "**3. Use Cases:**\n",
    "   - **When Spatial Information is Crucial:** Zero-padding is often used in tasks where spatial information at the edges of the image is critical, such as in object detection and semantic segmentation. It ensures that objects near the borders of the image are adequately processed.\n",
    "\n",
    "### **Valid-Padding:**\n",
    "\n",
    "**1. Definition:**\n",
    "   - **Valid-Padding:** No padding is added to the input image or feature map.\n",
    "\n",
    "**2. Effect on Output Feature Map:**\n",
    "   - **Reduces Spatial Dimensions:** Without padding, convolutional operations lead to a reduction in spatial dimensions. The output feature map is smaller than the input feature map, especially when using large convolutional kernels or multiple layers of convolution.\n",
    "\n",
    "   - **Loss of Border Information:** Valid-padding does not preserve information at the borders of the input. Features at the edges and corners may not be entirely captured, potentially leading to a loss of valuable details.\n",
    "\n",
    "   - **Efficiency:** Valid-padding is computationally more efficient as it reduces the number of computations and memory requirements. However, it comes at the cost of potentially losing edge information.\n",
    "\n",
    "**3. Use Cases:**\n",
    "   - **When Computational Efficiency is a Priority:** Valid-padding is used in situations where computational resources are limited, and sacrificing some edge information is acceptable. Tasks like image classification, where central features are more critical, can benefit from valid-padding.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "- **Zero-Padding:** Preserves spatial dimensions, centers convolution, mitigates information loss, and provides control over feature map size.\n",
    "  \n",
    "- **Valid-Padding:** Reduces spatial dimensions, may lose border information, and is computationally efficient.\n",
    "\n",
    "The choice between zero-padding and valid-padding depends on the specific task and the importance of edge information. In applications where edge details are crucial, zero-padding is preferred to ensure that spatial relationships and features at the borders of the input are appropriately captured. In tasks where computational efficiency is a priority and edge details are less critical, valid-padding can be used to reduce computations and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1b143",
   "metadata": {},
   "source": [
    "# TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6dfc1",
   "metadata": {},
   "source": [
    "## Ans : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b7481",
   "metadata": {},
   "source": [
    "**Overview of LeNet-5 Architecture:**\n",
    "\n",
    "LeNet-5 is a pioneering convolutional neural network (CNN) designed for handwritten digit recognition. Developed by Yann LeCun and his collaborators in the 1990s, it played a crucial role in popularizing the concept of deep learning and convolutional neural networks.\n",
    "\n",
    "**Key Components of LeNet-5:**\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - LeNet-5 takes grayscale images of size 32x32 pixels as input. Unlike modern datasets like ImageNet, MNIST digits are relatively small, and LeNet-5's architecture is tailored for this size.\n",
    "\n",
    "2. **Convolutional Layers:**\n",
    "   - The network consists of two convolutional layers. The first convolutional layer applies 6 filters of size 5x5, resulting in feature maps.\n",
    "   - The second convolutional layer applies 16 filters of size 5x5 to the feature maps generated by the first layer.\n",
    "\n",
    "3. **Activation Functions:**\n",
    "   - LeNet-5 primarily uses the sigmoid activation function. In modern architectures, ReLU (Rectified Linear Unit) is more common due to its faster convergence, but at the time of LeNet-5's creation, sigmoid was a prevalent choice.\n",
    "\n",
    "4. **Pooling Layers:**\n",
    "   - After each convolutional layer, LeNet-5 incorporates average pooling layers. Pooling is done with 2x2 windows, reducing the spatial dimensions of the feature maps.\n",
    "\n",
    "5. **Fully Connected Layers:**\n",
    "   - Following the convolutional and pooling layers, LeNet-5 has three fully connected layers. The first two fully connected layers consist of 120 and 84 neurons, respectively.\n",
    "   - The final fully connected layer consists of 10 neurons, corresponding to the 10 possible classes (digits 0-9 in the case of MNIST).\n",
    "\n",
    "6. **Softmax Activation:**\n",
    "   - The output layer uses the softmax activation function, transforming the network's raw output into probability scores for each class. The class with the highest probability is considered the network's prediction.\n",
    "\n",
    "**Advantages and Contributions:**\n",
    "\n",
    "1. **Parameter Sharing:** LeNet-5 introduced the concept of parameter sharing in convolutional layers. Sharing parameters among different regions of the input significantly reduces the number of trainable parameters, making the network more efficient.\n",
    "\n",
    "2. **Spatial Hierarchies:** By using multiple layers of convolution and pooling, LeNet-5 captures spatial hierarchies of features. Lower layers focus on simple edges and textures, while deeper layers combine these features into more complex patterns.\n",
    "\n",
    "3. **Robustness to Translations:** Due to its convolutional nature, LeNet-5 is inherently robust to translations. It can recognize patterns regardless of their position in the input image.\n",
    "\n",
    "4. **Inspiration for Modern Networks:** LeNet-5 laid the groundwork for modern CNN architectures. Concepts like convolutional layers, pooling layers, and fully connected layers are fundamental components of many contemporary deep learning models.\n",
    "\n",
    "While LeNet-5's architecture might appear simplistic compared to more recent networks, its design principles and innovations remain influential in the field of computer vision and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95c5c7",
   "metadata": {},
   "source": [
    "## Ans : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6a301",
   "metadata": {},
   "source": [
    "Certainly, let's break down the key components of LeNet-5 and their respective purposes:\n",
    "\n",
    "### **1. Convolutional Layers:**\n",
    "- **Purpose:** The primary function of convolutional layers is to extract features from the input images or feature maps. The convolution operation involves sliding small filters (kernels) over the input, capturing patterns like edges, textures, and more complex structures.\n",
    "\n",
    "### **2. Activation Functions (Sigmoid in LeNet-5):**\n",
    "- **Purpose:** Activation functions introduce non-linearities into the network, allowing it to learn complex patterns. In LeNet-5, sigmoid activation functions were used, allowing the network to model non-linear relationships in the data. Modern networks often use ReLU (Rectified Linear Unit) due to its faster convergence.\n",
    "\n",
    "### **3. Average Pooling Layers:**\n",
    "- **Purpose:** Pooling layers down-sample the spatial dimensions of the feature maps, reducing computational complexity and the number of parameters in the network. Average pooling computes the average value within a window, helping in capturing general patterns and reducing sensitivity to small variations in the input.\n",
    "\n",
    "### **4. Fully Connected Layers:**\n",
    "- **Purpose:** Fully connected layers process the features extracted by convolutional and pooling layers and make predictions. These layers learn complex combinations of features and map them to the output classes. In LeNet-5, there are two fully connected layers before the output layer.\n",
    "\n",
    "### **5. Softmax Activation (Output Layer):**\n",
    "- **Purpose:** The softmax activation function is used in the output layer to transform the network's raw output into probabilities. Each neuron in the output layer represents the probability of a specific class. Softmax ensures that the probabilities sum up to 1, allowing the network to make a confident prediction about the input image's class.\n",
    "\n",
    "### **6. Parameter Sharing:**\n",
    "- **Purpose:** LeNet-5 introduced the concept of parameter sharing in convolutional layers. By using the same set of weights for multiple regions of the input, the network learns to recognize various patterns with a reduced number of parameters. This parameter sharing significantly reduces the computational burden and enhances the network's ability to generalize to different patterns.\n",
    "\n",
    "### **7. Stride and Padding:**\n",
    "- **Purpose:** Stride determines how much the convolutional kernel moves during each step, affecting the output size. Padding adds extra pixels around the input, ensuring that the convolutional kernel covers the entire input, preserving edge information. Both stride and padding configurations influence the spatial dimensions of the feature maps and the network's ability to capture different patterns.\n",
    "\n",
    "### **8. Spatial Hierarchies:**\n",
    "- **Purpose:** Through multiple layers of convolution and pooling, LeNet-5 captures spatial hierarchies of features. Lower layers focus on simple patterns, while deeper layers combine these patterns into more complex structures. This hierarchical feature extraction is essential for recognizing intricate patterns and objects in images.\n",
    "\n",
    "In summary, the key components of LeNet-5 work together to extract hierarchical features from input images, reduce spatial dimensions, introduce non-linearities, and make predictions about the input's class. The architecture's innovative design, including parameter sharing and hierarchical feature extraction, contributed significantly to the development of modern convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862bb6d",
   "metadata": {},
   "source": [
    "## Ans : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cccddb",
   "metadata": {},
   "source": [
    "**Advantages of LeNet-5 :**\n",
    "\n",
    "1. **Efficiency in Small Datasets:** LeNet-5 is efficient for small image datasets like MNIST due to its relatively simple architecture. It strikes a balance between complexity and performance, making it suitable for tasks where large datasets are not available.\n",
    "\n",
    "2. **Parameter Sharing:** LeNet-5 introduced the concept of parameter sharing in convolutional layers. This reduces the number of trainable parameters, making the model computationally efficient and reducing the risk of overfitting, especially in small datasets.\n",
    "\n",
    "3. **Translation Invariance:** The use of convolutional layers ensures translation invariance, allowing the network to recognize patterns regardless of their position in the image. This property is crucial for tasks like digit recognition where the location of the digit varies within the image.\n",
    "\n",
    "4. **Hierarchical Feature Extraction:** LeNet-5 employs multiple layers of convolution and pooling, enabling the network to extract hierarchical features. Lower layers capture simple patterns like edges, while deeper layers combine these patterns into more complex structures. This hierarchical feature extraction is essential for recognizing intricate patterns in images.\n",
    "\n",
    "**Limitations of LeNet-5 :**\n",
    "\n",
    "1. **Limited to Small and Simple Datasets:** LeNet-5's architecture is not deep or complex enough to handle large, diverse datasets like ImageNet effectively. It struggles with more intricate patterns and diverse object categories, limiting its applicability to broader image recognition tasks.\n",
    "\n",
    "2. **Sigmoid Activation:** LeNet-5 uses sigmoid activation functions, which suffer from the vanishing gradient problem. This issue can slow down the training process, making it difficult to train deep networks effectively. Modern architectures use ReLU or its variants, which mitigate this problem.\n",
    "\n",
    "3. **Lack of Flexibility:** Due to its fixed architecture, LeNet-5 lacks the flexibility to adapt to the varying complexities of different image classification tasks. More recent architectures, like ResNet and Inception networks, use skip connections and parallel pathways, allowing them to adapt better to different types of features and patterns.\n",
    "\n",
    "4. **Overlooking Spatial Relationships:** While LeNet-5 captures local spatial relationships, it doesn't consider global spatial structures in the image. Understanding broader contexts in images is crucial for tasks like scene recognition, where the arrangement of objects plays a significant role.\n",
    "\n",
    "5. **Limited to 2D Spatial Information:** LeNet-5, like many early CNNs, processes images as 2D grids of pixels. It doesn't inherently capture 3D spatial relationships, limiting its use in tasks where understanding the depth or volumetric information in images is essential.\n",
    "\n",
    "In summary, LeNet-5 is advantageous for small, simple datasets where its efficiency, translation invariance, and hierarchical feature extraction capabilities shine. However, its limitations become apparent when dealing with more complex tasks and datasets. Modern architectures have evolved to address these limitations, incorporating advanced activation functions, deeper networks, and innovative design elements to handle a wider range of image classification challenges effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47939fbe",
   "metadata": {},
   "source": [
    "## Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c45bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.1884 - accuracy: 0.9423 - val_loss: 0.0743 - val_accuracy: 0.9759\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0640 - accuracy: 0.9804 - val_loss: 0.0680 - val_accuracy: 0.9774\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0462 - accuracy: 0.9853 - val_loss: 0.0461 - val_accuracy: 0.9849\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0360 - accuracy: 0.9886 - val_loss: 0.0416 - val_accuracy: 0.9862\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0299 - accuracy: 0.9903 - val_loss: 0.0379 - val_accuracy: 0.9879\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.0430 - val_accuracy: 0.9862\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0206 - accuracy: 0.9935 - val_loss: 0.0321 - val_accuracy: 0.9898\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0175 - accuracy: 0.9945 - val_loss: 0.0497 - val_accuracy: 0.9849\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.0579 - val_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.0366 - val_accuracy: 0.9898\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0366 - accuracy: 0.9898\n",
      "Test accuracy: 0.989799976348877\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape the data to fit LeNet-5 architecture (32x32)\n",
    "x_train = tf.pad(x_train, [[0, 0], [2, 2], [2, 2]])\n",
    "x_test = tf.pad(x_test, [[0, 0], [2, 2], [2, 2]])\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, (5, 5), activation='relu', input_shape=(32, 32, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(tf.expand_dims(x_train, axis=-1), y_train, epochs=10, validation_data=(tf.expand_dims(x_test, axis=-1), y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(tf.expand_dims(x_test, axis=-1), y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21755e8",
   "metadata": {},
   "source": [
    "<h2>Insights and Observations:</h2><br>\n",
    "Training Time: LeNet-5 is a relatively shallow network, so it trains relatively quickly even on a CPU. You can observe that it reaches decent accuracy within a few epochs.\n",
    "\n",
    "Accuracy: LeNet-5 should achieve decent accuracy on the MNIST dataset, often reaching above 99% accuracy after training. However, due to its simplicity, it might not perform as well on more complex datasets.\n",
    "\n",
    "Model Size: LeNet-5 has a relatively small number of parameters compared to modern architectures, resulting in a compact model size. This can be an advantage in memory-constrained environments.\n",
    "\n",
    "Visualizations: You can visualize the learned features in the convolutional layers to understand what kind of patterns the network is recognizing. This can provide valuable insights into the network's inner workings.\n",
    "\n",
    "Limitation: Due to its simple architecture, LeNet-5 might struggle with more complex datasets that contain intricate patterns and variations. For such datasets, more advanced architectures like VGG, ResNet, or Inception are generally preferred.\n",
    "\n",
    "Further Experimentation: You can experiment with hyperparameters like learning rate, number of epochs, and batch size to observe their effects on the training process and final accuracy.\n",
    "\n",
    "By training and evaluating LeNet-5 on MNIST, you can gain valuable hands-on experience with one of the foundational CNN architectures and understand its strengths and limitations in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e22459",
   "metadata": {},
   "source": [
    "# TOPIC: Analyzing AlexNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e3c3cc",
   "metadata": {},
   "source": [
    "## Ans : 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828cdb5",
   "metadata": {},
   "source": [
    "**Overview of AlexNet Architecture:**\n",
    "\n",
    "AlexNet is a deep convolutional neural network that achieved significant success in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, it marked a breakthrough in deep learning and played a crucial role in popularizing convolutional neural networks (CNNs) in computer vision tasks. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "### **1. Input Layer:**\n",
    "- AlexNet takes RGB images as input with dimensions 224x224x3 (height, width, channels) where channels represent the Red, Green, and Blue color channels.\n",
    "\n",
    "### **2. Convolutional Layers:**\n",
    "- **First Convolutional Layer:** 96 kernels of size 11x11 with a stride of 4 pixels. Applies ReLU activation and followed by max-pooling with a 3x3 window and a stride of 2 pixels.\n",
    "- **Second Convolutional Layer:** 256 kernels of size 5x5. Applies ReLU activation and followed by max-pooling with a 3x3 window and a stride of 2 pixels.\n",
    "- **Third Convolutional Layer:** 384 kernels of size 3x3. Applies ReLU activation.\n",
    "- **Fourth and Fifth Convolutional Layers:** Both have 384 kernels of size 3x3. Fourth layer applies ReLU activation, while the fifth layer applies ReLU and is followed by max-pooling with a 3x3 window and a stride of 2 pixels.\n",
    "\n",
    "### **3. Fully Connected Layers:**\n",
    "- After the convolutional layers, there are two fully connected layers with 4096 neurons each, both followed by ReLU activation. Dropout is applied to these layers to prevent overfitting.\n",
    "- The final fully connected layer has 1000 neurons (representing 1000 ImageNet classes) with softmax activation for classification.\n",
    "\n",
    "### **4. Other Architectural Innovations:**\n",
    "- **ReLU Activation:** AlexNet extensively used Rectified Linear Units (ReLU) as activation functions instead of traditional sigmoid or tanh. ReLU helps mitigate the vanishing gradient problem and accelerates convergence during training.\n",
    "- **Local Response Normalization (LRN):** Applied after the first and second convolutional layers, LRN normalizes the activations within a local neighborhood. It enhances the model's ability to generalize and respond to various inputs.\n",
    "- **Overlapping Max-Pooling:** Max-pooling layers with overlapping regions were used. Overlapping pooling regions help capture more spatial hierarchies in the features.\n",
    "\n",
    "### **5. Architectural Innovations' Impact:**\n",
    "- AlexNet's design innovations, including the use of ReLU activation, overlapping max-pooling, and dropout, contributed to its success in improving the accuracy of image classification tasks.\n",
    "- Its deep architecture allowed it to learn intricate features and patterns from large-scale datasets, paving the way for more complex and deeper CNN architectures in the future.\n",
    "\n",
    "In summary, AlexNet's architectural innovations and its success in the ILSVRC significantly influenced the development of deep learning models. Its principles are still foundational in the design of modern CNN architectures for various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20578c8b",
   "metadata": {},
   "source": [
    "## Ans : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c90da0",
   "metadata": {},
   "source": [
    "AlexNet introduced several architectural innovations that contributed to its breakthrough performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. These innovations played a crucial role in the advancement of deep learning models for computer vision tasks. Here are the key architectural innovations in AlexNet:\n",
    "\n",
    "### **1. Deep Architecture:**\n",
    "   - **Innovation:** AlexNet was one of the first deep convolutional neural networks (CNNs) with eight layers, five convolutional layers, and three fully connected layers.\n",
    "   - **Significance:** Deeper architectures allowed the network to learn complex hierarchical features from raw pixel values, enabling it to capture intricate patterns and variations in images.\n",
    "\n",
    "### **2. Rectified Linear Unit (ReLU) Activation Function:**\n",
    "   - **Innovation:** AlexNet used ReLU activation functions instead of traditional sigmoid or tanh functions.\n",
    "   - **Significance:** ReLU activation mitigates the vanishing gradient problem, allowing the network to converge faster during training. It accelerates learning by enabling the model to learn from large datasets more effectively.\n",
    "\n",
    "### **3. Overlapping Max-Pooling:**\n",
    "   - **Innovation:** AlexNet used max-pooling layers with overlapping regions.\n",
    "   - **Significance:** Overlapping pooling regions helped capture more spatial hierarchies in the features. It increased the model's ability to recognize patterns at different scales and positions, enhancing its robustness.\n",
    "\n",
    "### **4. Local Response Normalization (LRN):**\n",
    "   - **Innovation:** LRN was applied after the first and second convolutional layers.\n",
    "   - **Significance:** LRN normalizes activations within a local neighborhood, enhancing the contrast between activated neurons. It encouraged the model to focus on more discriminative features, making the network more accurate and robust.\n",
    "\n",
    "### **5. Dropout Regularization:**\n",
    "   - **Innovation:** Dropout was applied to the fully connected layers during training.\n",
    "   - **Significance:** Dropout randomly drops out (sets to zero) a fraction of neurons during training. It prevents overfitting by ensuring that the network does not rely too heavily on specific neurons. This regularization technique improved the model's generalization ability.\n",
    "\n",
    "### **6. GPU Acceleration:**\n",
    "   - **Innovation:** AlexNet was trained using graphics processing units (GPUs) for parallel processing.\n",
    "   - **Significance:** GPUs significantly accelerated the training process, allowing the model to handle the large amount of computation required by deep networks. This acceleration enabled faster experimentation and training on large datasets.\n",
    "\n",
    "### **7. Large Scale and Diverse Dataset:**\n",
    "   - **Innovation:** AlexNet was trained on the ImageNet dataset, which contained millions of labeled images spanning thousands of object categories.\n",
    "   - **Significance:** Training on a large and diverse dataset allowed the network to learn rich and generalized features, enabling it to recognize a wide range of objects in real-world scenarios.\n",
    "\n",
    "In summary, the architectural innovations introduced in AlexNet, such as deep layers, ReLU activation, overlapping max-pooling, LRN, dropout regularization, and GPU acceleration, collectively contributed to its breakthrough performance. These innovations became fundamental principles in the design of subsequent deep learning architectures, paving the way for the development of highly accurate and sophisticated models for various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67d50c",
   "metadata": {},
   "source": [
    "## Ans : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68973fad",
   "metadata": {},
   "source": [
    "**Role of Convolutional Layers, Pooling Layers, and Fully Connected Layers in AlexNet:**\n",
    "\n",
    "### **1. Convolutional Layers:**\n",
    "- **Role:** Convolutional layers in AlexNet are responsible for feature extraction. They apply convolution operations to the input image, detecting various patterns, edges, and textures. The layers learn hierarchical features, starting from simple patterns in the earlier layers to complex object parts in deeper layers.\n",
    "- **Innovation in AlexNet:** AlexNet's first convolutional layer used a large 11x11 kernel, allowing it to capture broader features in the input images. Subsequent convolutional layers used smaller kernels (5x5 and 3x3) to capture more fine-grained details.\n",
    "\n",
    "### **2. Pooling Layers:**\n",
    "- **Role:** Pooling layers perform down-sampling by reducing the spatial dimensions of the feature maps. Max-pooling, as used in AlexNet, selects the maximum value from a local region, effectively preserving the most activated features while reducing spatial resolution. Pooling helps the network focus on essential features and reduces computational complexity.\n",
    "- **Innovation in AlexNet:** AlexNet employed overlapping max-pooling, where the pooling regions overlap, capturing more spatial hierarchies in the features. Overlapping pooling contributed to the network's robustness and improved feature learning.\n",
    "\n",
    "### **3. Fully Connected Layers:**\n",
    "- **Role:** Fully connected layers process high-level features extracted by convolutional and pooling layers. These layers learn complex combinations of features and map them to the output classes. In AlexNet, there are two layers with 4096 neurons each, followed by a final layer with 1000 neurons for ImageNet's 1000 classes.\n",
    "- **Innovation in AlexNet:** The large number of neurons in the fully connected layers allowed AlexNet to learn intricate patterns and relationships in the features. Dropout regularization was applied to these layers, preventing overfitting by randomly deactivating neurons during training.\n",
    "\n",
    "### **Role of Softmax Activation (Output Layer):**\n",
    "- **Role:** The softmax activation function is applied in the output layer of AlexNet. It transforms the raw output scores into probabilities, indicating the likelihood of the input belonging to each class. The class with the highest probability is considered the network's prediction.\n",
    "- **Innovation in AlexNet:** While not specific to AlexNet, softmax activation in the output layer allowed the model to make multi-class predictions in the context of the ImageNet dataset.\n",
    "\n",
    "### **Impact of these Components:**\n",
    "- **Hierarchy of Features:** The combination of convolutional layers, pooling layers, and fully connected layers allowed AlexNet to learn a hierarchical representation of features, enabling it to recognize objects at different levels of abstraction.\n",
    "- **Capacity for Complexity:** The deep architecture and the high number of parameters in the fully connected layers gave AlexNet the capacity to understand intricate patterns and relationships in images, essential for its success on the ImageNet dataset.\n",
    "- **Generalization and Robustness:** The innovative design choices, such as overlapping pooling and dropout regularization, contributed to the model's generalization ability, making it robust to variations in the input data and preventing overfitting.\n",
    "\n",
    "In summary, the convolutional layers extract features, pooling layers reduce spatial dimensions, and fully connected layers learn complex patterns and relationships. These components, combined with innovative design choices, allowed AlexNet to achieve breakthrough performance in image recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84203124",
   "metadata": {},
   "source": [
    "## Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ecf4bef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"max_pooling2d_36\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node max_pooling2d_36/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,2,2,256].\n\nCall arguments received by layer \"max_pooling2d_36\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create AlexNet model architecture with reduced pooling layers\u001b[39;00m\n\u001b[0;32m      9\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10 classes for CIFAR-10\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1751\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1748\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1750\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1751\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;66;03m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;66;03m# TF_Operation.\u001b[39;00m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_traceback:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"max_pooling2d_36\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node max_pooling2d_36/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,2,2,256].\n\nCall arguments received by layer \"max_pooling2d_36\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create AlexNet model architecture with reduced pooling layers\n",
    "input_shape = (64,64, 3)  \n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1af69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
