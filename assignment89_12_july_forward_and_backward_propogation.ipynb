{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b79837c",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "Forward propagation is the process by which input data is passed through the neural network to generate an output prediction. It involves computing the weighted sum of inputs, applying activation functions, and passing the information through the layers until the output layer is reached. The purpose of forward propagation is to calculate the predicted output of the neural network for a given input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ce9b1",
   "metadata": {},
   "source": [
    "## Ans : 2 \n",
    "\n",
    "In a single-layer feedforward neural network, the output y is calculated as the weighted sum of inputs x plus a bias term b, passed through an activation function f. Mathematically, it can be represented as:\n",
    "\n",
    "y=f(wx+b)\n",
    "where \n",
    "\n",
    "w represents the weights, \n",
    "\n",
    "x is the input, \n",
    "\n",
    "b is the bias, and \n",
    "\n",
    "f is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232ada0",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    "Activation functions introduce non-linearities into the neural network, allowing it to learn complex patterns in the data. They squash the output of each neuron to a certain range. Common activation functions include sigmoid, tanh, and ReLU (Rectified Linear Unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ea467",
   "metadata": {},
   "source": [
    "## Ans : 4 \n",
    "\n",
    "Weights (w) determine the strength of the connections between neurons, indicating how much influence one neuron has on the next. Biases (b) allow the neuron to adjust the output based on the input data. During forward propagation, inputs are multiplied by weights, summed up, the bias is added, and the result is passed through an activation function to produce the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1275c",
   "metadata": {},
   "source": [
    "## Ans : 5\n",
    "\n",
    "Softmax function is used in the output layer for multi-class classification problems. It converts raw scores (logits) into probabilities. Softmax ensures that the output values are between 0 and 1 and that they sum up to 1, representing probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faaf98",
   "metadata": {},
   "source": [
    "## Ans : 6 \n",
    "\n",
    "Backward propagation is the process of updating the weights and biases of the neural network based on the calculated loss. It involves computing the gradient of the loss function with respect to the weights and biases, which is used to update the parameters and minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59528d15",
   "metadata": {},
   "source": [
    "## Ans : 7\n",
    "\n",
    "In a single-layer neural network, the gradients can be calculated using the chain rule. The gradient of the loss with respect to the weights (∂w/\n",
    "∂L ) is calculated as the product of the gradient of the loss with respect to the output (∂y/∂L) and the gradient of the output with respect to the weights (∂w/∂y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff6275",
   "metadata": {},
   "source": [
    "## Ans : 8 \n",
    "\n",
    "The chain rule states that the derivative of a composite function is the derivative of the outer function evaluated at the inner function, multiplied by the derivative of the inner function. In the context of neural networks, it is used to calculate the gradients of the loss function with respect to the weights and biases in order to update them during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0143519",
   "metadata": {},
   "source": [
    "## Ans : 9 \n",
    "\n",
    "Some common challenges during backward propagation include vanishing gradients, exploding gradients, and numerical stability issues. Techniques like using appropriate activation functions (like ReLU to mitigate vanishing gradients), gradient clipping (to address exploding gradients), and careful weight initialization can help mitigate these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bc3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
