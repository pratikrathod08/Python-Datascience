{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac52933b",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "Hierarchical clustering and its differences from other clustering techniques:\n",
    "    \n",
    "Hierarchical clustering is a type of clustering algorithm that builds a tree-like structure (dendrogram) of nested clusters. It differs from other clustering techniques in that it does not require specifying the number of clusters beforehand and provides a visual representation of how data points are merged into clusters at different similarity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebbb03",
   "metadata": {},
   "source": [
    "## Ans : 2\n",
    "\n",
    "Two main types of hierarchical clustering algorithms:\n",
    "\n",
    "a) Agglomerative Hierarchical Clustering (Bottom-up): It starts with each data point as a separate cluster and iteratively merges the closest clusters until all data points belong to a single cluster. The result is a dendrogram showing the merging process.\n",
    "\n",
    "b) Divisive Hierarchical Clustering (Top-down): It starts with all data points in one cluster and recursively divides the cluster into smaller clusters until each data point is in its cluster. The result is a dendrogram showing the division process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a201b44",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    "The distance between two clusters is calculated based on a distance metric. Common distance metrics used are:\n",
    "\n",
    "a) Euclidean Distance: Measures the straight-line distance between two data points in a multidimensional space.\n",
    "\n",
    "b) Manhattan Distance: Measures the sum of absolute differences between the coordinates of two data points.\n",
    "\n",
    "c) Pearson Correlation: Measures the linear correlation between two data points' features.\n",
    "\n",
    "d) Cosine Similarity: Measures the cosine of the angle between two data points in a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f7626",
   "metadata": {},
   "source": [
    "## Ans : 4\n",
    "\n",
    "To determine the optimal number of clusters, we can use methods like:\n",
    "\n",
    "a) Dendrogram: Visually inspect the dendrogram to identify significant jumps in the inter-cluster distance, which can indicate the optimal number of clusters.\n",
    "\n",
    "b) Cophenetic Correlation Coefficient: Measures how faithfully the dendrogram preserves pairwise distances. Higher values indicate a better clustering structure.\n",
    "\n",
    "c) Elbow Method: Plot the variance (or squared error) against different cluster numbers and look for the \"elbow point.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e6b38e",
   "metadata": {},
   "source": [
    "## Ans : 5\n",
    "\n",
    "A dendrogram is a tree-like diagram representing the hierarchical structure of clusters. It shows the order of merging/dividing clusters and the distance at which the merging/dividing occurs. Dendrograms are useful in analyzing the results because they allow us to:\n",
    "\n",
    "Visually inspect the clustering hierarchy and determine the optimal number of clusters.\n",
    "Identify clusters at different levels of similarity.\n",
    "Understand the relationships and similarities between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660a891",
   "metadata": {},
   "source": [
    "## Ans : 6\n",
    "\n",
    "Hierarchical clustering for numerical and categorical data:\n",
    "\n",
    "Yes, hierarchical clustering can be used for both numerical and categorical data. However, the distance metrics differ for each type:\n",
    "\n",
    "For numerical data, commonly used distance metrics are Euclidean distance, Manhattan distance, etc.\n",
    "For categorical data, metrics like Jaccard distance or Hamming distance can be used to measure dissimilarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5949d4c",
   "metadata": {},
   "source": [
    "## Ans : 7\n",
    "\n",
    "Using hierarchical clustering to identify outliers or anomalies:\n",
    "\n",
    "Hierarchical clustering can indirectly identify outliers by isolating them into their clusters. Outliers are more likely to form separate branches or be assigned to singleton clusters. By setting an appropriate threshold for the inter-cluster distance, we can identify outliers that have a significantly larger distance to the nearest cluster centroid.\n",
    "\n",
    "Please note that you need to implement these concepts in your Jupyter notebook to complete the assignment. Once you have completed your assignment, you can upload it to GitHub and share the repository link as per your instructor's guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad609c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
