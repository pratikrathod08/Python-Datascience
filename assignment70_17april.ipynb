{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e81956",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "Gradient Boosting Regression is a variant of the gradient boosting algorithm that is specifically designed for regression problems. It is an ensemble method that combines multiple weak regression models (typically decision trees) to create a strong regression model. Gradient Boosting Regression iteratively trains weak regression models to correct the errors made by the previous models. The algorithm minimizes a loss function by updating the model's parameters in the direction of the negative gradient of the loss function with respect to the predicted values. The final prediction is obtained by summing the predictions of all weak models, weighted by their learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e22d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ans : 2 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df=sns.load_dataset('tips')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525ba2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoding data \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "column=['sex','smoker','day','time']\n",
    "df[column]=df[column].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7276c022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>29.03</td>\n",
       "      <td>5.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>27.18</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>22.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>17.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>18.78</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip  sex  smoker  day  time  size\n",
       "0         16.99  1.01    0       0    2     0     2\n",
       "1         10.34  1.66    1       0    2     0     3\n",
       "2         21.01  3.50    1       0    2     0     3\n",
       "3         23.68  3.31    1       0    2     0     2\n",
       "4         24.59  3.61    0       0    2     0     4\n",
       "..          ...   ...  ...     ...  ...   ...   ...\n",
       "239       29.03  5.92    1       0    1     0     3\n",
       "240       27.18  2.00    0       1    1     0     2\n",
       "241       22.67  2.00    1       1    1     0     2\n",
       "242       17.82  1.75    1       0    1     0     2\n",
       "243       18.78  3.00    0       0    3     0     2\n",
       "\n",
       "[244 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d486a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dependent and independent features \n",
    "\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4c3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y ,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6127a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.14584163124467\n",
      "0.526518091174655\n"
     ]
    }
   ],
   "source": [
    "## import model \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regressior = GradientBoostingRegressor()\n",
    "regressior.fit(X_train , y_train )\n",
    "\n",
    "y_pred = regressior.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "print(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans : 3\n",
    "\n",
    "parameters={\n",
    "    'learning_rate':[0.1,0.01,0.001,0.0001],\n",
    "    'n_estimators':[10,20,50,100],\n",
    "    'max_depth':[1,10,20,30,40,50,100]\n",
    "    \n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7950484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model_gc = GridSearchCV(model,param_grid=parameters,cv=5)\n",
    "model_rc = RandomizedSearchCV(model,param_distributions=parameters,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f918e771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "                   param_distributions={'learning_rate': [0.1, 0.01, 0.001,\n",
       "                                                          0.0001],\n",
       "                                        'max_depth': [1, 10, 20, 30, 40, 50,\n",
       "                                                      100],\n",
       "                                        'n_estimators': [10, 20, 50, 100]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gc.fit(X_train , y_train)\n",
    "model_rc.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202ab872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eab8555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50, 'max_depth': 1, 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d696ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4676488744647454"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21f54eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46287357041400234"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900cac5",
   "metadata": {},
   "source": [
    "## Ans : 4\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a base model that is typically simple and has low predictive power on its own. In the context of regression, weak learners are often decision trees with shallow depth or limited complexity. These weak learners are trained to make predictions on the residuals (i.e., the differences between the actual target values and the current model's predictions). By sequentially adding multiple weak learners and combining their predictions, Gradient Boosting gradually improves its predictive performance and builds a strong ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96e222",
   "metadata": {},
   "source": [
    "## Ans : 5\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively build an ensemble of weak learners that collectively form a strong learner. The algorithm focuses on minimizing the residuals (errors) made by the current model by training subsequent weak learners to correct those residuals. By repeatedly updating the model's parameters in the direction that minimizes the loss function's gradient, the algorithm gradually reduces the overall error and improves its predictive capability. The learning process emphasizes the instances that are difficult to predict correctly, allowing the model to focus on the areas where it performs poorly. This iterative boosting and correcting process results in a highly accurate and robust ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f5913",
   "metadata": {},
   "source": [
    "## Ans : 6\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners through a sequential process. The basic steps involved in constructing the ensemble are as follows:\n",
    "\n",
    "Initialize the ensemble with an initial prediction, typically the mean of the target variable.\n",
    "\n",
    "Compute the residuals by subtracting the ensemble's current prediction from the actual target values.\n",
    "\n",
    "Train a weak learner (e.g., decision tree) on the residuals. The weak learner is typically a shallow tree with limited depth or complexity.\n",
    "\n",
    "Update the ensemble by adding the weak learner's predictions, weighted by a learning rate.\n",
    "\n",
    "Repeat steps 2-4 for a specified number of iterations, each time training a new weak learner on the residuals of the previous iteration.\n",
    "\n",
    "The final prediction of the ensemble is obtained by summing the predictions of all weak learners, weighted by their learning rates.\n",
    "\n",
    "By iteratively training weak learners on the residuals and updating the ensemble, the Gradient Boosting algorithm progressively improves its predictive power and builds a strong learner from a collection of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898722",
   "metadata": {},
   "source": [
    "## Ans : 7 \n",
    "\n",
    "The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm are as follows:\n",
    "\n",
    "Define a loss function that quantifies the error between the predicted values and the actual target values.\n",
    "\n",
    "Initialize the ensemble with an initial prediction, usually the mean of the target variable.\n",
    "\n",
    "Compute the negative gradient of the loss function with respect to the current prediction. This gradient represents the direction and magnitude of the update needed to reduce the loss.\n",
    "\n",
    "Train a weak learner (e.g., decision tree) to predict the negative gradient, treating it as the target variable. The weak learner aims to capture the patterns and relationships in the data that can help improve the ensemble's prediction.\n",
    "\n",
    "Compute the learning rate, which controls the contribution of each weak learner to the ensemble. The learning rate is a hyperparameter that determines the weight assigned to each weak learner's prediction.\n",
    "\n",
    "Update the ensemble by adding the weak learner's prediction, multiplied by the learning rate. This update adjusts the ensemble's prediction in the direction of the negative gradient, aiming to reduce the loss.\n",
    "\n",
    "Repeat steps 3-6 for a specified number of iterations, each time training a new weak learner on the negative gradients of the previous iteration and updating the ensemble.\n",
    "\n",
    "The final prediction of the ensemble is obtained by summing the predictions of all weak learners, weighted by their learning rates.\n",
    "\n",
    "By iteratively updating the ensemble based on the negative gradients and combining the predictions of weak learners, the Gradient Boosting algorithm minimizes the loss function and builds a strong learner capable of making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89717730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
