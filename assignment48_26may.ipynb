{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef0bcf3",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "Simple Linear Regression: Simple linear regression involves predicting a dependent variable based on a single independent variable. It assumes a linear relationship between the two variables and aims to find the best-fitting line that minimizes the sum of squared differences between the predicted and actual values. An example of simple linear regression is predicting house prices based on the area of the house.\n",
    "\n",
    "Multiple Linear Regression: Multiple linear regression extends the concept of simple linear regression by involving multiple independent variables to predict a dependent variable. It assumes a linear relationship between the dependent variable and multiple independent variables. The goal is to find the best-fitting hyperplane that minimizes the sum of squared differences. An example of multiple linear regression is predicting a person's salary based on their age, education level, and years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b30de",
   "metadata": {},
   "source": [
    "## Ans : 2 \n",
    "\n",
    "Assumptions of Linear Regression:\n",
    "\n",
    "Linearity: The relationship between the dependent variable and independent variables is linear.\n",
    "Independence: Observations are independent of each other.\n",
    "Homoscedasticity: The variance of errors is constant across all levels of the independent variables.\n",
    "Normality: The errors are normally distributed.\n",
    "No Multicollinearity: The independent variables are not highly correlated with each other.\n",
    "To check whether these assumptions hold in a given dataset, you can use the following techniques:\n",
    "\n",
    "Scatter plots: Examine scatter plots of the dependent variable against each independent variable to assess linearity.\n",
    "Residual plots: Plot the residuals (errors) against the predicted values to check for homoscedasticity.\n",
    "Normality tests: Perform statistical tests, such as the Shapiro-Wilk test or Q-Q plots, to assess the normality of residuals.\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable to detect multicollinearity. VIF values above a certain threshold indicate high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e355e",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    "In a linear regression model (y = mx + c), the slope (m) represents the change in the dependent variable (y) for a one-unit change in the independent variable (x). It indicates the rate of change in y with respect to x. The sign of the slope indicates whether the relationship is positive or negative.\n",
    "\n",
    "The intercept (c) represents the estimated value of the dependent variable (y) when the independent variable (x) is zero. It is the point where the regression line intersects the y-axis.\n",
    "\n",
    "Example: Suppose we have a linear regression model to predict the sales revenue (y) based on the advertising expenditure (x). If the estimated slope is 0.5, it means that for every one-unit increase in advertising expenditure, the sales revenue is expected to increase by 0.5 units. If the intercept is 100, it means that when there is zero advertising expenditure, the estimated sales revenue is 100 units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5004ff",
   "metadata": {},
   "source": [
    "## Ans : 4\n",
    "\n",
    "Gradient descent is an optimization algorithm used in machine learning to minimize the cost function of a model. It is commonly used in training models with a large number of parameters.\n",
    "\n",
    "The concept of gradient descent involves iteratively adjusting the model's parameters (coefficients) in the direction that reduces the cost function. It starts with an initial guess of the parameter values and updates them in small steps proportional to the negative gradient of the cost function. The learning rate determines the step size in each iteration.\n",
    "\n",
    "By repeatedly updating the parameters based on the gradient of the cost function, gradient descent aims to find the optimal parameter values that minimize the cost and improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31a34d",
   "metadata": {},
   "source": [
    "## Ans : 5\n",
    "\n",
    "Multiple linear regression is a regression model that involves multiple independent variables (features) to predict a dependent variable. It extends the concept of simple linear regression by capturing the combined effect of multiple predictors on the dependent variable.\n",
    "\n",
    "The multiple linear regression model can be represented as:\n",
    "y = b0 + b1x1 + b2x2 + ... + bn*xn\n",
    "\n",
    "Where:\n",
    "\n",
    "y is the dependent variable.\n",
    "b0 is the intercept.\n",
    "b1, b2, ..., bn are the coefficients representing the effect of each independent variable (x1, x2, ..., xn).\n",
    "x1, x2, ..., xn are the independent variables.\n",
    "The goal of multiple linear regression is to estimate the coefficient values that best fit the data and allow for accurate prediction of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3422af7",
   "metadata": {},
   "source": [
    "## Ans : 6\n",
    "\n",
    "Multicollinearity refers to a high correlation between independent variables in a multiple linear regression model. It can pose challenges in the interpretation and stability of the model. When multicollinearity exists, it becomes difficult to discern the individual effects of the correlated independent variables on the dependent variable.\n",
    "\n",
    "Detection of multicollinearity:\n",
    "\n",
    "Correlation matrix: Examine the correlation matrix to identify high correlation coefficients between independent variables.\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF values above a certain threshold (often 5 or 10) indicate high collinearity.\n",
    "Addressing multicollinearity:\n",
    "\n",
    "Feature selection: Remove one or more correlated variables from the model.\n",
    "Principal Component Analysis (PCA): Transform the correlated variables into a new set of uncorrelated variables.\n",
    "Ridge regression or Lasso regression: Apply regularization techniques that can mitigate the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93eb35c",
   "metadata": {},
   "source": [
    "## Ans : 7\n",
    "\n",
    "Polynomial regression is a form of regression analysis that allows for nonlinear relationships between the independent and dependent variables. It extends the concept of linear regression by introducing polynomial terms (powers) of the independent variable.\n",
    "\n",
    "The polynomial regression model can be represented as:\n",
    "y = b0 + b1x + b2x^2 + ... + bn*x^n\n",
    "\n",
    "Where:\n",
    "\n",
    "y is the dependent variable.\n",
    "b0, b1, b2, ..., bn are the coefficients representing the effect of each polynomial term.\n",
    "x is the independent variable.\n",
    "n is the degree of the polynomial.\n",
    "Polynomial regression fits a curve to the data by incorporating higher-order polynomial terms. It can capture more complex relationships that linear regression cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873fe2f5",
   "metadata": {},
   "source": [
    "## Ans : 8\n",
    "\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: It can model nonlinear relationships between variables, allowing for more accurate representation of complex data patterns.\n",
    "Fit to data: It can provide a better fit to the data compared to linear regression when the relationship is not strictly linear.\n",
    "Feature expansion: It allows the exploration of interactions and nonlinear effects between variables.\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: Polynomial regression with high-degree polynomials can be prone to overfitting, capturing noise and outliers in the data.\n",
    "Complexity: As the degree of the polynomial increases, the model becomes more complex, making it harder to interpret and prone to multicollinearity.\n",
    "Extrapolation: Extrapolating beyond the range of the observed data can lead to unreliable predictions.\n",
    "In situations where the relationship between variables is nonlinear or when there is prior knowledge that suggests polynomial terms would improve the model's performance, polynomial regression can be preferred over linear regression. However, it is important to consider the trade-offs between model complexity, interpretability, and the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b9f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
