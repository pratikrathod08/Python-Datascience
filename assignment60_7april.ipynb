{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce72208",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "Polynomial functions and linear functions are related in machine learning algorithms in the sense that polynomial functions can be represented using linear functions with appropriate transformations of the input features.\n",
    "\n",
    "In many cases, machine learning algorithms, such as linear regression or support vector machines (SVMs), use linear functions to model the relationships between input features and the target variable. A linear function is defined as a function where the output is a linear combination of the input features, typically represented as weights multiplied by the input features, summed together.\n",
    "\n",
    "However, the relationship between polynomial functions and linear functions arises through feature engineering. By introducing additional features that are powers or combinations of the original features, the linear model can capture non-linear relationships. This process is known as polynomial feature expansion.\n",
    "\n",
    "For example, consider a simple linear regression problem with one input feature `x` and a target variable `y`. In a linear model, the relationship between `x` and `y` is assumed to be linear, represented as `y = w * x + b`, where `w` and `b` are the weights and bias term, respectively.\n",
    "\n",
    "Now, suppose we want to capture non-linear relationships using polynomial functions. We can introduce additional features such as `x^2`, `x^3`, and so on. The linear model with polynomial features becomes `y = w1 * x + w2 * x^2 + w3 * x^3 + ... + b`. Although this is a linear function in terms of the weights and input features, it can effectively model non-linear relationships.\n",
    "\n",
    "Machine learning algorithms, such as polynomial regression or polynomial SVMs, can explicitly incorporate polynomial features or automatically generate them during training to capture non-linear patterns in the data.\n",
    "\n",
    "Therefore, while polynomial functions themselves are non-linear, they can be represented using linear functions through feature engineering, allowing linear models to capture non-linear relationships.\n",
    "\n",
    "It's important to note that using polynomial features can increase the complexity of the model and potentially lead to overfitting if not regularized appropriately.\n",
    "\n",
    "I hope this clarifies the relationship between polynomial functions and linear functions in machine learning algorithms. Let me know if you have any further questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928b4fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## Ans : 2\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)  # Use a degree of 3 for a cubic polynomial kernel\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d74118",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    "In Support Vector Regression (SVR), the parameter epsilon (Îµ) defines the margin of tolerance within which errors are considered acceptable. It controls the width of the \"tube\" around the regression line, within which errors are not penalized.\n",
    "\n",
    "Increasing the value of epsilon in SVR generally leads to an increase in the number of support vectors. This is because a larger epsilon allows more data points to be considered within the margin of tolerance.\n",
    "\n",
    "When epsilon is small, the SVR model aims to fit the data as closely as possible, potentially leading to a narrower tube and fewer support vectors. In this case, the model prioritizes minimizing the errors within a tight margin.\n",
    "\n",
    "On the other hand, when epsilon is large, the SVR model allows more data points to fall within the margin of tolerance, resulting in a wider tube. This flexibility in accommodating errors can lead to a larger number of support vectors.\n",
    "\n",
    "It's important to note that the number of support vectors affects the complexity and computational requirements of the SVR model. More support vectors can make the model more computationally expensive and potentially increase the risk of overfitting, especially if the dataset is small.\n",
    "\n",
    "Choosing an appropriate value for epsilon involves a trade-off between model complexity and the desired tolerance for errors. It depends on the specific problem and dataset you are working with. It is often a hyperparameter that needs to be tuned during the model development process.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR tends to increase the number of support vectors, as it allows more data points to be considered within the margin of tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3794ae6d",
   "metadata": {},
   "source": [
    "## Ans : 4\n",
    "\n",
    "Certainly! Let's discuss how the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR), along with examples of when you might want to increase or decrease their values.\n",
    "\n",
    "1. Kernel Function:\n",
    "   - Choice: SVR supports various kernel functions such as linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - Effect: The choice of kernel function determines the shape of the decision boundary or regression line.\n",
    "   - Examples:\n",
    "     - Linear Kernel: Use when the relationship between input features and target variable is expected to be linear.\n",
    "     - Polynomial Kernel: Use when there is a non-linear relationship with polynomial patterns in the data.\n",
    "     - RBF Kernel: Suitable for capturing complex non-linear relationships with localized patterns.\n",
    "     - Sigmoid Kernel: Applicable when there is a prior belief that the relationship follows a sigmoidal pattern.\n",
    "\n",
    "2. C Parameter:\n",
    "   - Effect: The C parameter controls the trade-off between fitting the training data and allowing errors. It determines the penalty for errors and the width of the margin.\n",
    "   - Examples:\n",
    "     - Increase C: Use when you want to enforce a stricter fit to the training data and reduce the number of support vectors. This may increase the risk of overfitting.\n",
    "     - Decrease C: Use when you want to allow more errors and have a wider margin, which can lead to a larger number of support vectors. This may increase the model's ability to generalize but with potentially lower accuracy.\n",
    "\n",
    "3. Epsilon Parameter:\n",
    "   - Effect: The epsilon parameter defines the margin of tolerance within which errors are considered acceptable. It controls the width of the tube around the regression line within which errors are not penalized.\n",
    "   - Examples:\n",
    "     - Increase Epsilon: Use when you want to allow larger errors or have a wider margin of tolerance. This can result in a larger number of support vectors and a more flexible model.\n",
    "     - Decrease Epsilon: Use when you want to reduce the tolerance for errors or have a narrower margin. This can lead to a smaller number of support vectors and a more rigid model.\n",
    "\n",
    "4. Gamma Parameter:\n",
    "   - Effect: The gamma parameter determines the influence of each training example on the decision boundary or regression line. It defines the reach of the kernel function and controls the smoothness of the decision boundary.\n",
    "   - Examples:\n",
    "     - Increase Gamma: Use when you want a more localized decision boundary or regression line. This makes the model focus on nearby points, potentially leading to overfitting if the dataset is small.\n",
    "     - Decrease Gamma: Use when you want a smoother decision boundary or regression line that takes into account a larger neighborhood of points. This can help the model generalize better but may result in underfitting if the dataset is complex.\n",
    "\n",
    "It's important to note that the impact of these parameters can vary depending on the specific dataset and problem. It's recommended to perform hyperparameter tuning and cross-validation to find the optimal values for these parameters for your specific task.\n",
    "\n",
    "I hope this explanation helps you understand the effects of kernel function, C parameter, epsilon parameter, and gamma parameter in SVR. Let me know if you have any further questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8c1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy score is :  1.0\n",
      "Best Parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best Score: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "## Ans : 5\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate performance \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\" Accuracy score is : \",accuracy_score(y_pred,y_test))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# train classifier with best parameters \n",
    "\n",
    "svc = SVC(C=1,gamma=0.1,kernel='linear')\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred=svc.predict(X_test)\n",
    "\n",
    "svc.fit(X,y)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('svc.pkl','wb') as model:\n",
    "    pickle.dump(svc,model)\n",
    "    model.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67169068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
