{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d172b851",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "The mathematical formulation for a linear Support Vector Machine (SVM) involves finding the optimal hyperplane that maximally separates the data points of different classes. The linear SVM aims to find a decision boundary in the form of a hyperplane in a higher-dimensional space. Here's the mathematical formulation:\n",
    "\n",
    "Given a training dataset with labeled samples:\n",
    "- Input features: X = {x₁, x₂, ..., xn} (each xᵢ is a vector representing a feature)\n",
    "- Corresponding labels: Y = {y₁, y₂, ..., yn} (each yᵢ is a binary class label)\n",
    "\n",
    "The goal is to find a hyperplane defined by a weight vector w and a bias term b that separates the data points into two classes with the maximum margin.\n",
    "\n",
    "The decision function for a linear SVM is:\n",
    "f(x) = sign(w^T x + b)\n",
    "\n",
    "The sign function returns +1 for one class and -1 for the other class. The SVM seeks to find the optimal values for w and b.\n",
    "\n",
    "The optimization problem of finding the optimal hyperplane can be formulated as:\n",
    "minimize: (1/2) * ||w||² + C * ∑ξ\n",
    "subject to: yᵢ(w^T xᵢ + b) ≥ 1 - ξᵢ for all data points (xᵢ, yᵢ)\n",
    "            ξᵢ ≥ 0 for all data points (xᵢ, yᵢ)\n",
    "\n",
    "In the objective function, the term (1/2) * ||w||² represents the regularization term to control the margin and prevent overfitting. The constant C is a hyperparameter that determines the trade-off between maximizing the margin and minimizing the classification error. The ∑ξ term is the sum of slack variables ξᵢ, which allow for misclassifications or samples that lie within the margin.\n",
    "\n",
    "The constraints ensure that the samples are correctly classified, and the margin violations are minimized. The goal is to minimize the objective function while satisfying the constraints.\n",
    "\n",
    "The optimization problem can be solved using various optimization techniques, such as quadratic programming or gradient descent, to find the optimal values for w and b that maximize the margin and minimize the classification error.\n",
    "\n",
    "Note: This formulation applies to the case of linearly separable data. For non-linearly separable data, techniques such as the kernel trick can be used to map the data into a higher-dimensional space where a linear SVM can find a separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e0a64",
   "metadata": {},
   "source": [
    "## Ans : 2\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is to minimize the hinge loss while also regularizing the weights. The hinge loss represents the classification error or margin violations of the SVM, while regularization helps control the complexity of the model. The objective function is defined as follows:\n",
    "\n",
    "minimize: (1/2) * ||w||² + C * ∑ξ\n",
    "\n",
    "In this objective function:\n",
    "- ||w||² represents the squared Euclidean norm of the weight vector w. This term is the regularization term and helps control the complexity of the model. Minimizing ||w||² encourages a smaller margin and a more complex decision boundary.\n",
    "- C is a hyperparameter that determines the trade-off between maximizing the margin and minimizing the classification error. It adjusts the importance of the regularization term relative to the hinge loss term. A smaller C emphasizes a larger margin at the cost of potential misclassifications, while a larger C focuses more on minimizing the classification error.\n",
    "- ∑ξ represents the sum of slack variables ξᵢ, which allow for misclassifications or samples that lie within the margin. The term ∑ξ measures the total hinge loss or classification error of the SVM. The objective is to minimize this term to reduce misclassifications.\n",
    "\n",
    "The objective function combines the regularization term (||w||²) and the hinge loss term (C * ∑ξ). By minimizing this objective function, the SVM seeks to find the optimal values for the weight vector w that maximize the margin and minimize the classification error. The hyperparameter C controls the balance between these two objectives.\n",
    "\n",
    "The optimization problem aims to find the values of w that satisfy the objective function while also satisfying the constraints of correctly classifying the training samples. Various optimization techniques, such as quadratic programming or gradient descent, can be employed to solve this optimization problem and find the optimal values of w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594a2ce",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to handle non-linearly separable data by implicitly mapping the data points into a higher-dimensional feature space. It allows SVMs to find a linear decision boundary in this higher-dimensional space, effectively solving non-linear classification problems. The kernel trick avoids the need to explicitly compute the coordinates of the data points in the higher-dimensional space, which can be computationally expensive.\n",
    "\n",
    "In SVMs, the kernel trick is applied by replacing the dot product between input feature vectors with a kernel function. The kernel function calculates the similarity or inner product of the feature vectors in the original input space or implicitly in a higher-dimensional feature space.\n",
    "\n",
    "The general form of the decision function using the kernel trick is:\n",
    "f(x) = sign(∑ αᵢ yᵢ K(x, xᵢ) + b)\n",
    "\n",
    "In this equation:\n",
    "- αᵢ is the Lagrange multiplier associated with each training sample.\n",
    "- yᵢ is the class label of the training sample.\n",
    "- K(x, xᵢ) is the kernel function that measures the similarity or inner product between the feature vectors x and xᵢ.\n",
    "- b is the bias term.\n",
    "\n",
    "The kernel function allows the SVM to implicitly project the input data into a higher-dimensional space where a linear decision boundary can be found, even when the original data is not linearly separable.\n",
    "\n",
    "Commonly used kernel functions include:\n",
    "- Linear Kernel: K(x, xᵢ) = x^T xᵢ\n",
    "- Polynomial Kernel: K(x, xᵢ) = (γ(x^T xᵢ) + r)^d\n",
    "- Gaussian (RBF) Kernel: K(x, xᵢ) = exp(-γ||x - xᵢ||²)\n",
    "- Sigmoid Kernel: K(x, xᵢ) = tanh(γ(x^T xᵢ) + r)\n",
    "\n",
    "By applying the kernel trick, SVMs can efficiently solve non-linear classification problems by implicitly mapping the data points into a higher-dimensional feature space. This technique allows for powerful and flexible classification while avoiding the computational cost of explicitly operating in the higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15a092",
   "metadata": {},
   "source": [
    "## Ans : 4\n",
    "\n",
    "In Support Vector Machines (SVMs), support vectors play a crucial role in defining the decision boundary and determining the classification of data points. Support vectors are the data points that lie closest to the decision boundary, also known as the margin. These points have the most influence on the construction of the decision boundary and the overall performance of the SVM.\n",
    "\n",
    "The role of support vectors in SVMs can be explained with examples:\n",
    "\n",
    "1. Linearly separable case:\n",
    "   Suppose we have two classes, represented by red and blue points, that are linearly separable by a straight line. The support vectors in this case are the data points that lie on or closest to the margin or the decision boundary. These support vectors define the position and orientation of the decision boundary.\n",
    "\n",
    "\n",
    "   In the example above, the red and blue circles represent the support vectors. The decision boundary is defined by the line that separates these support vectors. The support vectors determine the width of the margin, and any misclassification or movement of these points could potentially change the decision boundary.\n",
    "\n",
    "2. Non-linearly separable case:\n",
    "   When dealing with non-linearly separable data, SVMs use the kernel trick to implicitly map the data into a higher-dimensional feature space where linear separation is possible. In this case, the support vectors play a crucial role in defining the decision boundary in the higher-dimensional space.\n",
    "\n",
    "   In the example above, the data points are not linearly separable in the original feature space (2D), but they can be separated by a circle in the implicit higher-dimensional feature space. The support vectors are the data points that lie on or closest to the margin or the decision boundary in the higher-dimensional space. These support vectors define the shape and position of the decision boundary.\n",
    "\n",
    "   It's important to note that the SVM is a sparse model, meaning that only the support vectors contribute to the decision boundary. The remaining data points that are not support vectors do not impact the decision boundary directly.\n",
    "\n",
    "By identifying the support vectors and considering their positions relative to the decision boundary, SVMs can make predictions on new data points efficiently and accurately. The support vectors guide the construction of the decision boundary, making SVMs a powerful tool for both linearly and non-linearly separable classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138fe34",
   "metadata": {},
   "source": [
    "## Ans : 5\n",
    "\n",
    "Sure! Let's start by defining each of these terms:\n",
    "\n",
    "1. Hyperplane: In machine learning and geometry, a hyperplane is a subspace of one dimension less than its ambient space. In the context of support vector machines (SVMs), a hyperplane is a decision boundary that separates different classes of data.\n",
    "\n",
    "2. Soft Margin: In SVM, a soft margin allows for some misclassifications in order to achieve a more flexible decision boundary. It allows data points to be on the wrong side of the decision boundary, but penalizes them with a certain cost.\n",
    "\n",
    "3. Hard Margin: In contrast to a soft margin, a hard margin SVM aims to find a decision boundary that perfectly separates the two classes without any misclassifications. It is more strict and less tolerant of misclassifications.\n",
    "\n",
    "4. Marginal Plane: The marginal plane is the hyperplane that is closest to the support vectors in an SVM. Support vectors are the data points that are closest to the decision boundary.\n",
    "\n",
    "Let's illustrate these concepts with a simple example:\n",
    "\n",
    "Suppose we have a two-dimensional dataset with two classes, represented by blue and red points. We want to find a decision boundary (hyperplane) to separate these classes.\n",
    "\n",
    "Here's an example graph:\n",
    "\n",
    "```\n",
    "       +----------------------------------------+\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |               red points                |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       +----------------------------------------+\n",
    "                         |\n",
    "                  Decision Boundary\n",
    "                         |\n",
    "       +----------------------------------------+\n",
    "       |                                        |\n",
    "       |               blue points               |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       +----------------------------------------+\n",
    "```\n",
    "\n",
    "In the graph, the blue and red points represent the two classes. The decision boundary (hyperplane) is the line that separates the blue and red points. In the case of a soft margin SVM, some misclassifications might occur, allowing points to be on the wrong side of the decision boundary.\n",
    "\n",
    "Here's an example of a soft margin SVM with a decision boundary:\n",
    "\n",
    "```\n",
    "       +----------------------------------------+\n",
    "       |                                        |\n",
    "       |               red points                |\n",
    "       |                                        |\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "       |                 / | \\                  |\n",
    "       |                /  |  \\                 |\n",
    "       |               |   |   |                |\n",
    "       |                \\  |  /                 |\n",
    "       |                 \\ | /                  |\n",
    "       |                   |                    |\n",
    "       |                                        |\n",
    "       +----------------------------------------+\n",
    "                         |\n",
    "                  Decision Boundary\n",
    "                         |\n",
    "       +----------------------------------------+\n",
    "       |                                        |\n",
    "       |               blue points               |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       +----------------------------------------+\n",
    "```\n",
    "\n",
    "In this case, some red points are on the blue side of the decision boundary, and some blue points are on the red side, but the SVM allows for these misclassifications.\n",
    "\n",
    "In contrast, a hard margin SVM aims to find a decision boundary that perfectly separates the two classes without any misclassifications. Here's an example of a hard margin SVM with a decision boundary:\n",
    "\n",
    "```\n",
    "       +----------------------------------------+\n",
    "       |                                        |\n",
    "       |               red points                |\n",
    "       |                                        |\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "      \n",
    "\n",
    "       |                   |                    |\n",
    "       |                   |                    |\n",
    "       +----------------------------------------+\n",
    "                         |\n",
    "                  Decision Boundary\n",
    "                         |\n",
    "       +----------------------------------------+\n",
    "       |                                        |\n",
    "       |               blue points               |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       |                                        |\n",
    "       +----------------------------------------+\n",
    "```\n",
    "\n",
    "In this case, there are no misclassifications, and the decision boundary perfectly separates the two classes.\n",
    "\n",
    "The marginal plane refers to the hyperplane that is closest to the support vectors. Support vectors are the data points that are closest to the decision boundary. In the above examples, the marginal plane would be the decision boundary itself.\n",
    "\n",
    "I hope this helps to clarify the concepts of hyperplane, soft margin, hard margin, and marginal plane! Let me know if you have any further questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1147b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ans : 6\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "data=datasets.load_iris()\n",
    "X=data.data\n",
    "y=data.target\n",
    "\n",
    "## train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "\n",
    "## train linear svm classifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "linear_svc=LinearSVC()\n",
    "\n",
    "linear_svc.fit(X_train,y_train)\n",
    "\n",
    "y_pred=linear_svc.predict(X_test)\n",
    "\n",
    "## Check accuracy \n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "print(accuracy_score(y_pred,y_test))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a667c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## regularize model \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LinearSVC classifier with regularization\n",
    "svm = LinearSVC(C=0.5)  # Adjust the C value as desired\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0047f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
