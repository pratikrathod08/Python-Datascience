{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4e88ab",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "\n",
    "\n",
    "ANOVA (Analysis of Variance) is a statistical method used to compare means between two or more groups. To use ANOVA effectively and ensure the validity of the results, certain assumptions need to be met. Here are the key assumptions required for ANOVA:\n",
    "\n",
    "1. Independence: The observations within each group and between groups should be independent of each other. This means that the measurements or data points should not be influenced by or correlated with each other.\n",
    "\n",
    "2. Normality: The data within each group should follow a normal distribution. Normality assumption is important because ANOVA relies on the normal distribution for its statistical inference. Violations of this assumption could lead to inaccurate results.\n",
    "\n",
    "3. Homogeneity of variances: The variances of the data within each group should be equal (homogeneity of variances). It means that the spread or dispersion of the data points around the mean should be similar across all groups. Unequal variances can affect the accuracy of the ANOVA results.\n",
    "\n",
    "4. Homogeneity of regression slopes (for factorial ANOVA): In factorial ANOVA, if there are interactions between the independent variables, the effect of one independent variable on the dependent variable should be consistent across all levels of the other independent variable.\n",
    "\n",
    "Violations of these assumptions can impact the validity of ANOVA results. Some examples of violations and their consequences include:\n",
    "\n",
    "1. Violation of independence: If the observations within or between groups are not independent, it can lead to pseudoreplication or biased results. For example, if data points are taken from related individuals or correlated measurements over time, the assumption of independence may be violated.\n",
    "\n",
    "2. Violation of normality: If the data within groups do not follow a normal distribution, the p-values and confidence intervals obtained from ANOVA may be inaccurate. Non-normality can lead to Type I or Type II errors, affecting the overall validity of the results. Transforming the data or using non-parametric alternatives may be necessary in such cases.\n",
    "\n",
    "3. Violation of homogeneity of variances: Unequal variances across groups can lead to biased estimation of the group means and affect the statistical tests. When the assumption is violated, the p-values and confidence intervals obtained from ANOVA may be unreliable. Transformations or using robust methods can be considered as alternatives.\n",
    "\n",
    "4. Violation of homogeneity of regression slopes: If the effect of one independent variable on the dependent variable varies across the levels of another independent variable, it implies an interaction effect. Ignoring this violation can lead to incorrect interpretation of main effects and interactions, potentially distorting the overall conclusions.\n",
    "\n",
    "It is important to assess these assumptions and consider appropriate remedies or alternative analyses if violations occur to ensure the validity of ANOVA results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e21d3a",
   "metadata": {},
   "source": [
    "## Ans : 2\n",
    "\n",
    "The three types of ANOVA are:\n",
    "\n",
    "1. One-Way ANOVA: One-Way ANOVA is used when you have one categorical independent variable (factor) and one continuous dependent variable. It is used to compare means across three or more independent groups. For example, you might use One-Way ANOVA to compare the average scores of students in three different study groups (Group A, Group B, Group C) to determine if there are any significant differences in their performance.\n",
    "\n",
    "2. Two-Way ANOVA: Two-Way ANOVA is used when you have two categorical independent variables (factors) and one continuous dependent variable. It is used to analyze the main effects of each independent variable as well as any interaction effects between them. For example, you might use Two-Way ANOVA to examine the effects of both gender and age group on the average test scores of students.\n",
    "\n",
    "3. Three-Way ANOVA: Three-Way ANOVA is used when you have three categorical independent variables (factors) and one continuous dependent variable. It allows you to analyze the main effects of each independent variable as well as any interactions among them. Three-Way ANOVA is useful when studying the influence of multiple factors simultaneously. For instance, you might use Three-Way ANOVA to investigate the effects of temperature, humidity, and light exposure on the growth rate of plants.\n",
    "\n",
    "In summary, One-Way ANOVA is used when you have one independent variable, Two-Way ANOVA is used when you have two independent variables, and Three-Way ANOVA is used when you have three independent variables. The choice of ANOVA type depends on the research design and the specific variables under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b147ecc",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    "The partitioning of variance in ANOVA refers to the division of the total variation in the dependent variable into different components that can be attributed to different sources or factors. It allows us to understand how much of the total variation is explained by the independent variables or factors being studied and how much is due to random variation or error.\n",
    "\n",
    "In ANOVA, the total variation in the dependent variable is decomposed into three components:\n",
    "\n",
    "1. Between-group variation: This component of variance represents the variation in the dependent variable that is explained by the differences between the group means. It assesses the overall effect of the independent variable(s) on the dependent variable.\n",
    "\n",
    "2. Within-group variation: This component of variance represents the variation in the dependent variable within each group. It captures the random or error variation that cannot be attributed to the independent variable(s). It accounts for individual differences, measurement error, and other sources of variation.\n",
    "\n",
    "3. Total variation: This is the sum of the between-group and within-group variations. It represents the overall variation in the dependent variable without any specific breakdown into sources. It serves as a reference point against which the between-group and within-group variations are compared.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1. Effectiveness of the independent variable(s): By examining the between-group variation, ANOVA allows us to assess the extent to which the independent variable(s) have a significant effect on the dependent variable. It helps determine whether there are statistically significant differences between the group means.\n",
    "\n",
    "2. Magnitude of the effect: The proportion of the total variance explained by the between-group variation, known as the effect size, provides information about the strength of the relationship between the independent and dependent variables. It helps evaluate the practical significance of the findings.\n",
    "\n",
    "3. Interpretation of results: By understanding the partitioning of variance, researchers can interpret the results of ANOVA more accurately. They can determine the relative contributions of different factors and assess the significance of the observed differences.\n",
    "\n",
    "4. Identification of sources of variation: ANOVA helps identify the sources of variation that influence the dependent variable. It allows researchers to focus on the factors that have the most substantial impact and may guide further investigation or intervention.\n",
    "\n",
    "In summary, the partitioning of variance in ANOVA provides a framework to understand the relative contributions of different factors and the extent to which they explain the observed variation in the dependent variable. It aids in interpreting the results, assessing the effectiveness of independent variables, and identifying important sources of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d6a88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 13.0\n",
      "Explained Sum of Squares (SSE): 4.5\n",
      "Residual Sum of Squares (SSR): 8.5\n"
     ]
    }
   ],
   "source": [
    "## Ans : 4\n",
    "\n",
    "# To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python, you can utilize the `statsmodels` library, which provides functions for performing ANOVA calculations. Here's an example of how you can calculate these sums of squares:\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({'group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "                     'value': [10, 12, 8, 7, 11, 9]})\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('value ~ group', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Extract the sum of squares\n",
    "SST = anova_table['sum_sq']['group']\n",
    "SSE = anova_table['sum_sq']['Residual']\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", SST)\n",
    "print(\"Explained Sum of Squares (SSE):\", SSE)\n",
    "print(\"Residual Sum of Squares (SSR):\", SSR)\n",
    "\n",
    "\n",
    "# In this example, we have a DataFrame `data` containing the groups and corresponding values. We fit a one-way ANOVA model using the `ols` function from `statsmodels` and then obtain the ANOVA table using `sm.stats.anova_lm()`. The `anova_table` provides the sum of squares values for the group and residual.\n",
    "# By accessing the 'sum_sq' column of the ANOVA table and selecting the respective row for the group and residual, we can calculate the SST, SSE, and SSR.\n",
    "# Note that you need to have the `pandas` and `statsmodels` libraries installed to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b849ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect 1: 13.000000000000014\n",
      "Main Effect 2: 0.16666666666666857\n",
      "Interaction Effect: 4.333333333333317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\anova.py:138: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  (model.ssr / model.df_resid))\n"
     ]
    }
   ],
   "source": [
    "## Ans : 5\n",
    "\n",
    "# To calculate the main effects and interaction effects in a two-way ANOVA using Python, you can utilize the `statsmodels` library, which provides functions for performing ANOVA calculations. Here's an example of how you can calculate these effects:\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({'group1': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "                     'group2': ['X', 'Y', 'X', 'Y', 'X', 'Y'],\n",
    "                     'value': [10, 12, 8, 7, 11, 9]})\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('value ~ group1 + group2 + group1:group2', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Extract the main effects and interaction effect\n",
    "main_effect_1 = anova_table['sum_sq']['group1']\n",
    "main_effect_2 = anova_table['sum_sq']['group2']\n",
    "interaction_effect = anova_table['sum_sq']['group1:group2']\n",
    "\n",
    "print(\"Main Effect 1:\", main_effect_1)\n",
    "print(\"Main Effect 2:\", main_effect_2)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n",
    "\n",
    "# In this example, we have a DataFrame `data` containing two groups (`group1` and `group2`) and the corresponding values. We fit a two-way ANOVA model using the `ols` function from `statsmodels` and specify the formula as `'value ~ group1 + group2 + group1:group2'` to include both main effects (`group1` and `group2`) and the interaction effect (`group1:group2`).\n",
    "# We obtain the ANOVA table using `sm.stats.anova_lm()`, which provides the sum of squares values for each effect. By accessing the 'sum_sq' column of the ANOVA table and selecting the respective rows for the main effects and interaction effect, we can calculate the values.\n",
    "# Note that you need to have the `pandas` and `statsmodels` libraries installed to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4166a",
   "metadata": {},
   "source": [
    "## Ans : 6\n",
    "\n",
    "\n",
    "Based on the obtained F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, we can draw the following conclusions:\n",
    "\n",
    "1. Overall differences: The result indicates that there are significant differences between the groups based on the independent variable being studied. The p-value of 0.02 suggests that the probability of obtaining such a significant F-statistic by chance alone is only 0.02 (or 2%).\n",
    "\n",
    "2. Rejection of the null hypothesis: In hypothesis testing, the null hypothesis states that there are no significant differences between the group means. With a p-value of 0.02, which is less than the commonly chosen significance level of 0.05, we have enough evidence to reject the null hypothesis. This means that there is strong evidence to suggest that the group means are not all equal.\n",
    "\n",
    "3. Importance of the independent variable: The obtained F-statistic of 5.23 indicates that there is a meaningful amount of variability between the groups that can be attributed to the independent variable being studied. The higher the F-statistic, the more variability is explained by the independent variable.\n",
    "\n",
    "4. Post-hoc analyses: If the one-way ANOVA is significant, it may be useful to perform post-hoc analyses (e.g., Tukey's test or pairwise comparisons) to determine which specific groups differ significantly from each other. These additional tests can provide more detailed information about the nature and direction of the differences between the groups.\n",
    "\n",
    "In summary, based on an F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, we can conclude that there are significant differences between the groups. The results suggest that the independent variable has a meaningful impact on the dependent variable, and further post-hoc analyses can be conducted to explore specific group differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5f31e",
   "metadata": {},
   "source": [
    "## Ans : 7\n",
    "\n",
    "Handling missing data in a repeated measures ANOVA is an important consideration. The approach to handling missing data depends on the nature and pattern of missingness. Here are some common methods and their potential consequences:\n",
    "\n",
    "1. Complete Case Analysis (Listwise deletion): This method involves excluding any cases with missing data on any of the variables included in the analysis. It is the simplest approach but may result in a reduction of sample size and potential loss of information. This method assumes that missingness is completely random (MCAR: Missing Completely at Random).\n",
    "\n",
    "2. Pairwise Deletion: This approach analyzes all available data for each pairwise comparison, disregarding cases with missing values in specific variables. While it retains more cases compared to complete case analysis, it can result in different sample sizes for different comparisons. This method assumes that missingness is missing at random (MAR) or ignorable.\n",
    "\n",
    "3. Imputation: Imputation involves estimating missing values based on the available data. Common imputation methods include mean imputation, regression imputation, and multiple imputation. Imputation helps to retain sample size and preserve statistical power but introduces potential bias if the imputation model is misspecified or the assumptions of imputation are violated.\n",
    "\n",
    "4. Maximum Likelihood Estimation: With this approach, missing data is handled using a full information maximum likelihood (FIML) estimation method. FIML estimates the parameters while accounting for missing values, utilizing all available information in the data. This method provides unbiased estimates under the assumption of missing at random (MAR) or missingness that can be modeled.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include:\n",
    "\n",
    "1. Biased estimates: Complete case analysis and pairwise deletion may lead to biased estimates if the missing data is related to the variables under study or the outcomes being analyzed. The estimated effects may not accurately represent the population.\n",
    "\n",
    "2. Loss of power: Excluding cases with missing data (complete case analysis) or using pairwise deletion can result in a reduced sample size, leading to a loss of statistical power and potentially wider confidence intervals.\n",
    "\n",
    "3. Invalid inferences: If missingness is related to the variables being analyzed or outcomes of interest, using methods that assume missingness is completely random (MCAR) or missing at random (MAR) can lead to invalid inferences and incorrect conclusions.\n",
    "\n",
    "4. Assumptions and limitations: Each method has its assumptions and limitations. Imputation methods assume the data is missing at random (MAR) and require careful consideration and appropriate model specification. FIML estimation assumes missingness is MAR and may be sensitive to model assumptions.\n",
    "\n",
    "It is crucial to carefully assess the nature and pattern of missingness, consider the assumptions of each method, and select an appropriate approach to handle missing data in a repeated measures ANOVA analysis. Consultation with a statistician or methodologist is recommended to make informed decisions and ensure the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f52f17",
   "metadata": {},
   "source": [
    "## Ans : 8\n",
    "\n",
    "\n",
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to determine which specific group differences are significant. Here are some common post-hoc tests and when they are typically used:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD) test: Tukey's HSD test is used when you have three or more groups and want to compare all possible pairwise differences. It controls the family-wise error rate, meaning it maintains the overall Type I error rate across all comparisons. Tukey's HSD test is conservative and is appropriate when you want to conduct multiple pairwise comparisons.\n",
    "\n",
    "2. Bonferroni correction: The Bonferroni correction is a conservative approach to control the family-wise error rate. It adjusts the significance level (alpha) by dividing it by the number of comparisons being made. It is commonly used when conducting multiple comparisons to maintain the desired overall alpha level.\n",
    "\n",
    "3. Scheffe's test: Scheffe's test is a more liberal post-hoc test that controls the family-wise error rate across all possible comparisons. It is suitable for situations where you have specific a priori hypotheses or when there is a small sample size. Scheffe's test is less powerful than Tukey's HSD test but provides more liberal confidence intervals.\n",
    "\n",
    "4. Fisher's Least Significant Difference (LSD) test: Fisher's LSD test is an older post-hoc test that compares all possible pairwise differences. It does not control the family-wise error rate, making it less stringent compared to Tukey's HSD or Scheffe's test. Fisher's LSD test is suitable when you have specific a priori hypotheses or when the number of comparisons is limited.\n",
    "\n",
    "Here's an example situation where a post-hoc test might be necessary:\n",
    "\n",
    "Suppose a researcher conducts an ANOVA to compare the mean scores of four different treatment groups (Group A, Group B, Group C, and Group D) on a specific outcome measure. The ANOVA reveals a significant overall effect, indicating that there are differences between the groups. In this scenario, the researcher would need to perform post-hoc tests to determine which specific group differences are significant. For example, they might use Tukey's HSD test to conduct pairwise comparisons between all groups and identify which pairs differ significantly from each other in terms of their mean scores. This would provide a more detailed understanding of the specific group differences and help draw more precise conclusions from the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22688c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 83.82529572338473\n",
      "p-value: 2.6209485319425612e-12\n"
     ]
    }
   ],
   "source": [
    "## Ans : 9\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data\n",
    "diet_A = np.array([3.2, 2.5, 2.8, 2.9, 3.1, 2.7, 2.8, 2.6, 2.9, 3.0])\n",
    "diet_B = np.array([2.1, 2.4, 1.9, 1.8, 2.2, 2.3, 2.1, 2.5, 2.0, 2.2])\n",
    "diet_C = np.array([1.4, 1.9, 1.7, 1.8, 1.5, 1.6, 1.9, 1.7, 1.8, 1.6])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Report the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63962010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      df     sum_sq    mean_sq          F    PR(>F)\n",
      "Software             2.0  21.166667  10.583333  10.583333  0.010773\n",
      "Experience           1.0  21.333333  21.333333  21.333333  0.003620\n",
      "Software:Experience  2.0   1.166667   0.583333   0.583333  0.586816\n",
      "Residual             6.0   6.000000   1.000000        NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "## Ans : 10\n",
    "\n",
    "# To conduct a two-way ANOVA using Python to analyze the average time it takes to complete a task using three different software programs (A, B, and C) and considering employee experience level (novice vs. experienced), you can use the `statsmodels` library. Here's an example of how you can perform the analysis:\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({'Software': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                     'Experience': ['Novice', 'Novice', 'Novice', 'Experienced', 'Experienced', 'Experienced',\n",
    "                                    'Novice', 'Novice', 'Novice', 'Experienced', 'Experienced', 'Experienced'],\n",
    "                     'Time': [12, 15, 10, 9, 11, 8, 14, 13, 11, 10, 12, 9]})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Time ~ Software + Experience + Software:Experience', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Report the results\n",
    "print(anova_table)\n",
    "\n",
    "\n",
    "# In this example, we have a DataFrame `data` containing the software program used (`Software`), the experience level of the employee (`Experience`), and the time it took to complete the task (`Time`). We fit a two-way ANOVA model using the `ols` function from `statsmodels` and specify the formula as `'Time ~ Software + Experience + Software:Experience'` to include both main effects (`Software` and `Experience`) and the interaction effect (`Software:Experience`).\n",
    "# By calling `sm.stats.anova_lm()`, we obtain the ANOVA table that provides the F-statistics and p-values for the main effects and interaction effect.\n",
    "# The interpretation of the results would depend on the obtained F-statistics and p-values. If a factor (e.g., software program or experience level) has a significant main effect, it suggests that the factor has a significant overall impact on the time it takes to complete the task. If the interaction effect is significant, it indicates that the combination of the factors has a significant impact on the outcome, and the effects of the factors are not additive.\n",
    "# By examining the ANOVA table, you can identify the F-statistics and p-values for the main effects and interaction effect. The p-value indicates the probability of obtaining such a significant F-statistic by chance alone. A p-value less than the chosen significance level (e.g., 0.05) suggests a significant effect.\n",
    "# For example, if the ANOVA table shows a significant p-value for the main effect of Software (e.g., p < 0.05) and a non-significant p-value for the main effect of Experience (e.g., p > 0.05), it would indicate that the software program used has a significant effect on the task completion time, while the experience level does not have a significant effect. If the interaction effect between Software and Experience is significant (e.g., p < 0.05), it would suggest that the impact of software programs on task completion time varies depending on the experience level of the employee.\n",
    "# Careful interpretation of the results, considering the obtained F-statistics and p-values, is important to understand the effects and interactions between the software programs and employee experience level on task completion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3434f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -7.500093276041119\n",
      "p-value: 5.231499626241686e-09\n"
     ]
    }
   ],
   "source": [
    "## Ans : 11\n",
    "\n",
    "# To conduct a two-sample t-test using Python to compare the test scores of two groups (control group and experimental group) and determine if there are any significant differences, you can use the `scipy.stats` module. Here's an example of how you can perform the analysis:\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data\n",
    "control_group = np.array([75, 82, 78, 80, 85, 79, 81, 77, 83, 76, 80, 79, 84, 77, 81, 78, 82, 80, 76, 79])\n",
    "experimental_group = np.array([82, 88, 85, 89, 87, 84, 86, 83, 85, 86, 88, 84, 87, 82, 86, 83, 85, 88, 84, 85])\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Report the results\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "\n",
    "# In this example, we have test score data for two groups: the control group (traditional teaching method) and the experimental group (new teaching method). Each group has 20 observations (students). We calculate the t-statistic and p-value using the `ttest_ind()` function from `scipy.stats`. The function takes the test score data for the control and experimental groups as separate arguments.\n",
    "\n",
    "# The t-statistic measures the difference between the means of the two groups relative to the variation within each group. The p-value indicates the probability of obtaining such a significant t-statistic by chance alone.\n",
    "\n",
    "# After running the code, you will obtain the t-statistic and p-value. The interpretation of the results would depend on the obtained values. If the p-value is less than the chosen significance level (e.g., 0.05), it suggests that there is a significant difference in test scores between the two groups. In such a case, you would reject the null hypothesis and conclude that the new teaching method has a significant impact on test scores compared to the traditional teaching method.\n",
    "\n",
    "# If the results are significant, and you want to determine which specific group(s) differ significantly from each other, you can perform post-hoc tests such as Tukey's HSD test or Bonferroni correction. These tests will help identify pairwise differences between the control and experimental groups. However, note that post-hoc tests are typically used in situations with more than two groups. In this case, since you only have two groups, there is no need for a post-hoc test.\n",
    "\n",
    "# It's important to note that the example provided assumes independent observations between the two groups and assumes that the data follows a normal distribution. If these assumptions are violated, alternative non-parametric tests may be more appropriate, such as the Mann-Whitney U test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb547d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 128.74123989218322\n",
      "p-value: 1.0053492340396575e-26\n"
     ]
    }
   ],
   "source": [
    "## Ans : 12 \n",
    "\n",
    "# Since you mentioned that the researcher wants to analyze the average daily sales of three retail stores (Store A, Store B, and Store C) over 30 days, a repeated measures ANOVA may not be suitable for this scenario. Repeated measures ANOVA is typically used when the same individuals or subjects are measured multiple times under different conditions or treatments.\n",
    "\n",
    "# In your case, where you have different stores and each day represents a separate measurement, a repeated measures design is not applicable. Instead, you can use a one-way ANOVA to compare the average daily sales of the three stores.\n",
    "\n",
    "# Here's an example of how you can conduct a one-way ANOVA using Python to analyze the average daily sales:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data\n",
    "store_A_sales = np.array([100, 120, 95, 110, 105, 90, 115, 100, 115, 105, 95, 110, 100, 95, 105, 100, 110, 90, 100, 105, 115, 95, 90, 105, 100, 110, 115, 90, 95, 100])\n",
    "store_B_sales = np.array([85, 95, 80, 90, 100, 85, 90, 95, 100, 90, 85, 95, 80, 90, 85, 100, 90, 95, 85, 100, 95, 90, 80, 85, 95, 90, 100, 85, 90, 95])\n",
    "store_C_sales = np.array([70, 75, 80, 75, 70, 80, 75, 70, 80, 75, 80, 75, 70, 80, 75, 70, 80, 75, 80, 75, 70, 80, 75, 80, 75, 70, 80, 75, 80, 75])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "# Report the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "\n",
    "# In this example, we have daily sales data for three stores (Store A, Store B, and Store C) over 30 days. Each store's sales data is represented by a separate numpy array (`store_A_sales`, `store_B_sales`, `store_C_sales`). We calculate the F-statistic and p-value using the `f_oneway()` function from `scipy.stats`. The function takes the sales data for each store as separate arguments.\n",
    "\n",
    "# The F-statistic represents the ratio of between-group variability to within-group variability. The p-value indicates the probability of obtaining such a significant F-statistic by chance alone.\n",
    "\n",
    "# After running the code, you will obtain the F-statistic and p-value. The interpretation of the results would depend on the obtained values. If the p-value is less than the chosen significance level (e.g., 0.05), it suggests that there is a significant difference in the average daily sales between at least two of the stores. In such a case, you would reject the null hypothesis and conclude that there are significant differences in sales among the three stores.\n",
    "\n",
    "# You may also consider conducting post-hoc tests, such as Tukey's HSD test or Bonferroni correction, to determine which specific pairs of stores differ significantly in terms of their average daily sales. These\n",
    "\n",
    "#  tests can provide additional insights into the differences between the stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d845bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
