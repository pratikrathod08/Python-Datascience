{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8e8c95",
   "metadata": {},
   "source": [
    "## Part : 1 \n",
    "\n",
    "## Ans : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cd9dd",
   "metadata": {},
   "source": [
    "Regularization in the context of deep learning refers to techniques used to prevent overfitting, a common problem in machine learning where a model performs well on the training data but fails to generalize to unseen or new data. Overfitting occurs when a model learns the noise and specific patterns in the training data, instead of capturing the underlying true patterns that generalize well to new data.\n",
    "\n",
    "Regularization methods add a penalty term to the loss function that the model tries to minimize during training. This penalty discourages the model from fitting the training data too closely and helps prevent overfitting. There are several types of regularization techniques used in deep learning, including L1 and L2 regularization, dropout, and early stopping.\n",
    "\n",
    "1. **L1 and L2 Regularization:** L1 regularization adds the absolute values of the coefficients as a penalty term to the loss function, while L2 regularization adds the squared values of the coefficients. These techniques are also known as Lasso and Ridge regression in the context of linear regression. In deep learning, they help in reducing the complexity of the neural network by penalizing large weights.\n",
    "\n",
    "2. **Dropout:** Dropout is a regularization technique specific to neural networks. During training, randomly selected neurons are ignored, or \"dropped out.\" This means that their contribution to the activation of downstream neurons is temporarily removed on the forward pass, and any weight updates are not applied to the dropped out neurons during backpropagation. Dropout helps in preventing complex co-adaptations on training data and acts as a form of ensemble learning, where multiple models are trained and combined.\n",
    "\n",
    "3. **Early Stopping:** Early stopping involves monitoring the model's performance on a validation set during training. Training is stopped when the performance on the validation set starts to degrade, even if the performance on the training set continues to improve. This helps prevent the model from overfitting the training data.\n",
    "\n",
    "Regularization is crucial in deep learning for several reasons:\n",
    "\n",
    "- **Preventing Overfitting:** The primary purpose of regularization is to prevent overfitting, ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "- **Improving Generalization:** Regularization techniques help in building models that generalize well to new, unseen data, which is a fundamental goal in machine learning.\n",
    "\n",
    "- **Increasing Robustness:** Regularized models tend to be more robust to variations and noise in the data, making them more reliable in real-world scenarios.\n",
    "\n",
    "In summary, regularization techniques are essential in deep learning because they promote the development of models that not only fit the training data well but also generalize their learning to make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7864f51",
   "metadata": {},
   "source": [
    "## Ans : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1707c",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning, including in the context of deep learning. It refers to the balance between two types of errors that affect the performance of a machine learning model: bias and variance.\n",
    "\n",
    "- **Bias:** Bias refers to the error introduced by approximating a real-world problem, which may be extremely complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs, leading to underfitting. Underfit models perform poorly both on the training data and unseen data because they oversimplify the underlying patterns in the data.\n",
    "\n",
    "- **Variance:** Variance, on the other hand, refers to the model's sensitivity to small fluctuations or noise in the training dataset. High variance can cause the model to model the random noise in the training data, leading to overfitting. Overfit models perform well on the training data but poorly on unseen data because they have essentially memorized the training examples and cannot generalize to new, unseen examples.\n",
    "\n",
    "The tradeoff occurs because decreasing bias often increases variance and vice versa. A model with high bias and low variance might oversimplify the data, while a model with low bias and high variance might fit the noise in the data.\n",
    "\n",
    "Regularization techniques help address the bias-variance tradeoff by adding constraints to the optimization problem the model is solving. Here's how regularization techniques like L1 and L2 regularization work in this context:\n",
    "\n",
    "1. **L1 and L2 Regularization (Penalty on Weights):** L1 regularization adds the absolute values of the coefficients (weights) as a penalty term to the loss function, and L2 regularization adds the squared values of the coefficients. These penalties discourage the model from assigning too much importance to any particular feature. In other words, they constrain the weights, preventing them from becoming overly large. Large weights can lead to high variance because the model becomes too sensitive to the fluctuations in the training data. By penalizing large weights, regularization helps in reducing variance.\n",
    "\n",
    "2. **L2 Regularization (Weight Decay):** L2 regularization, also known as weight decay, not only helps in reducing variance but also aids in reducing bias. By penalizing large weights, L2 regularization encourages the model to use all features and prevents it from oversimplifying the problem, thereby reducing bias.\n",
    "\n",
    "3. **Dropout:** Dropout is a regularization technique specific to neural networks. During training, dropout randomly sets a fraction of the input units to 0 at each update, effectively ignoring them. This prevents complex co-adaptations of neurons, which can occur when neurons rely too much on the presence of other neurons. Dropout helps in reducing overfitting (high variance) by ensuring that no single neuron becomes overly specialized.\n",
    "\n",
    "By employing these regularization techniques, machine learning models, including neural networks, can strike a balance between bias and variance. Regularization encourages the model to find simpler patterns in the data without fitting the noise, ultimately improving the model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444be1ca",
   "metadata": {},
   "source": [
    "## Ans : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab493f61",
   "metadata": {},
   "source": [
    "Certainly! L1 and L2 regularization are two common techniques used to prevent overfitting in machine learning models, including neural networks. They differ in terms of how they calculate the penalty for large coefficients (weights) and their effects on the model.\n",
    "\n",
    "### L1 Regularization (Lasso Regularization):\n",
    "\n",
    "**Penalty Calculation:** In L1 regularization, the penalty added to the loss function is proportional to the absolute values of the model's coefficients. Mathematically, for a given weight \\(w_i\\), the L1 penalty term is \\(|w_i|\\). The total L1 penalty is the sum of the absolute values of all weights in the model: \\(\\sum{|w_i|}\\).\n",
    "\n",
    "**Effect on the Model:**\n",
    "1. **Sparse Solutions:** L1 regularization encourages sparsity in the model, meaning it tends to push irrelevant or less relevant features' coefficients to exactly zero. Sparse models are easier to interpret and can be useful when dealing with high-dimensional data where many features might not be relevant.\n",
    "\n",
    "2. **Feature Selection:** L1 regularization acts as an automatic feature selector, favoring models that use a subset of features while setting others to zero. It is beneficial when you suspect that only a few features are truly important for making predictions.\n",
    "\n",
    "### L2 Regularization (Ridge Regularization):\n",
    "\n",
    "**Penalty Calculation:** In L2 regularization, the penalty added to the loss function is proportional to the squared values of the model's coefficients. Mathematically, for a given weight \\(w_i\\), the L2 penalty term is \\(w_i^2\\). The total L2 penalty is the sum of the squared values of all weights in the model: \\(\\sum{w_i^2}\\).\n",
    "\n",
    "**Effect on the Model:**\n",
    "1. **No Sparsity:** Unlike L1 regularization, L2 regularization does not encourage sparsity. It keeps all features but tries to make the coefficients small. It effectively prevents any single feature from having an extremely large influence on the model's predictions.\n",
    "\n",
    "2. **Smoothing the Model:** L2 regularization smoothens the model by spreading the \"importance\" of features across multiple features. It helps in preventing overfitting by discouraging any particular feature from having an excessively large weight.\n",
    "\n",
    "### Differences in Summary:\n",
    "\n",
    "- **Penalty Calculation:** L1 penalty is based on the absolute values of coefficients, while L2 penalty is based on the squared values of coefficients.\n",
    "  \n",
    "- **Effect on Sparsity:** L1 regularization encourages sparsity by driving some feature weights to exactly zero. L2 regularization does not induce sparsity; it keeps all features but prevents any single feature from dominating the model.\n",
    "\n",
    "- **Use Cases:** L1 regularization is useful when you suspect that only a few features are relevant, and you want a sparse model with feature selection. L2 regularization is useful for preventing multicollinearity (correlation between features) and creating a smoother model without enforcing sparsity.\n",
    "\n",
    "In practice, a combination of L1 and L2 regularization (known as Elastic Net) is often used to take advantage of both techniques and create a balance between sparsity and smoothness in the model. The choice between L1 and L2 regularization depends on the specific problem and the characteristics of the data you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522171b1",
   "metadata": {},
   "source": [
    "## Ans : 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560af12",
   "metadata": {},
   "source": [
    "Regularization plays a crucial role in preventing overfitting and improving the generalization of deep learning models. Overfitting occurs when a model learns the noise and specific patterns in the training data, making it perform well on the training set but poorly on unseen data. Regularization techniques add constraints to the optimization problem that the model is solving, preventing it from becoming too complex and fitting the noise in the data. Here's how regularization helps in preventing overfitting and improving generalization in deep learning models:\n",
    "\n",
    "### 1. **Preventing Overfitting:**\n",
    "\n",
    "- **Penalizing Large Weights:** Regularization methods like L1 and L2 regularization penalize large weights in the model. Neural networks with large weights are more prone to fitting the noise in the data. By adding a penalty term to the loss function based on the magnitude of the weights, regularization discourages the model from assigning too much importance to any particular feature. This prevents the model from becoming overly complex and fitting the training data too closely.\n",
    "\n",
    "- **Dropout:** Dropout is a regularization technique specific to neural networks. During training, dropout randomly sets a fraction of the input units to 0 at each update, effectively ignoring them. This prevents complex co-adaptations of neurons, making the model more robust and preventing overfitting. Dropout acts as a form of ensemble learning, where different subnetworks are trained at each iteration, preventing reliance on specific features or neurons.\n",
    "\n",
    "- **Early Stopping:** While not a direct form of regularization, early stopping is another technique used to prevent overfitting. It involves monitoring the model's performance on a validation set during training and stopping the training process when the performance on the validation set starts to degrade. This prevents the model from training for too many epochs and overfitting the training data.\n",
    "\n",
    "### 2. **Improving Generalization:**\n",
    "\n",
    "- **Encouraging Simplicity:** Regularization methods encourage the model to find simpler patterns in the data. By penalizing overly complex models, regularization helps the model focus on the most important features and relationships within the data. Simpler models are more likely to generalize well to unseen data because they capture the underlying trends rather than the noise.\n",
    "\n",
    "- **Handling Noisy Data:** Real-world data often contains noise and irrelevant features. Regularization helps in filtering out this noise and identifying the true underlying patterns. By preventing the model from fitting the noise, regularization ensures that the model's predictions are based on meaningful correlations in the data, leading to better generalization.\n",
    "\n",
    "- **Enabling Smoother Decision Boundaries:** In the case of neural networks, regularization techniques like weight decay (L2 regularization) encourage smoother decision boundaries. Smoother decision boundaries are less likely to be influenced by small fluctuations in the data and are more robust, leading to improved generalization.\n",
    "\n",
    "In summary, regularization techniques act as a form of control that prevents deep learning models from becoming overly complex and fitting the training data too closely. By encouraging simplicity, discouraging noise, and promoting smoother decision boundaries, regularization techniques significantly contribute to preventing overfitting and improving the generalization ability of deep learning models, making them more reliable for making predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fc133",
   "metadata": {},
   "source": [
    "## Part : 2 \n",
    "\n",
    "## Ans : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b644a5d",
   "metadata": {},
   "source": [
    "**Dropout Regularization:**\n",
    "\n",
    "Dropout is a regularization technique specific to neural networks, introduced to prevent overfitting. During training, dropout randomly sets a fraction of the input units (neurons) to 0 at each update, effectively \"dropping out\" these units from the network for that iteration. The dropout rate, usually a small fraction like 0.2 or 0.5, represents the probability that each neuron is dropped out during a training iteration. This dropout process is applied independently to each unit.\n",
    "\n",
    "**How Dropout Reduces Overfitting:**\n",
    "\n",
    "1. **Ensemble Learning Effect:** Dropout can be seen as a form of ensemble learning, where multiple models are trained on different subsets of the data. During each training iteration, a different random subset of neurons is dropped out, leading the network to learn different features from different parts of the input space. At test time, all neurons are used, but their weights are scaled to account for the dropout during training. This ensemble effect prevents the network from relying too much on specific neurons or features, reducing overfitting.\n",
    "\n",
    "2. **Reduces Co-Adaptations:** Neurons in a neural network can develop complex co-adaptations, where they rely on the presence of other specific neurons. By dropping out random subsets of neurons, dropout breaks these co-adaptations. Neurons must become more robust and learn useful features in the absence of other specific neurons, leading to a more generalized and robust model.\n",
    "\n",
    "**Impact on Model Training and Inference:**\n",
    "\n",
    "1. **Training:** During training, dropout is applied to the input and hidden layers. It effectively makes the training process noisy and prevents the network from memorizing the training data. Each forward and backward pass involves a different subset of neurons, making the network more adaptable and preventing overfitting. Dropout increases the training time because multiple forward and backward passes are required for each training iteration.\n",
    "\n",
    "2. **Inference:** During inference or when making predictions on new data, dropout is not applied. All neurons are used, but their weights are scaled down by the dropout rate. This scaling ensures that the expected output of each neuron remains the same as during training, accounting for the dropped-out neurons. Inference is typically faster than training because dropout is not involved, and the entire network is used to make predictions.\n",
    "\n",
    "In summary, dropout regularization reduces overfitting by preventing the network from relying too heavily on specific neurons and features during training. By introducing noise and breaking co-adaptations, dropout encourages the network to learn more generalized and robust representations of the data. During training, dropout creates an ensemble of different subnetworks, while during inference, the full network is used without dropout, ensuring accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6a46e",
   "metadata": {},
   "source": [
    "## Ans : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f740591",
   "metadata": {},
   "source": [
    "**Early Stopping as a Form of Regularization:**\n",
    "\n",
    "Early stopping is a regularization technique used in machine learning to prevent overfitting during the training process. It involves monitoring the model's performance on a validation dataset during training and stopping the training process when the performance on the validation dataset starts to degrade, even if the performance on the training dataset continues to improve. By halting the training early, before the model becomes too tailored to the training data, early stopping helps prevent overfitting.\n",
    "\n",
    "**How Early Stopping Prevents Overfitting:**\n",
    "\n",
    "1. **Detection of Overfitting:** During the training process, a model's performance on the training data usually improves over time. However, if the model starts to memorize the training data (overfitting), its performance on a separate validation dataset (which it has not seen during training) may start to degrade. Early stopping leverages this phenomenon.\n",
    "\n",
    "2. **Preventing Excessive Training:** Early stopping interrupts the training process when it detects that the model's performance on the validation data is no longer improving. This prevents the model from becoming too specialized to the training data, ensuring that it generalizes well to new, unseen data.\n",
    "\n",
    "3. **Simpler Models:** By stopping the training early, the resulting model is typically simpler. It has seen fewer examples and has fewer parameters adjusted to fit the training data perfectly. Simpler models are less prone to overfitting because they capture the essential patterns in the data without memorizing the noise.\n",
    "\n",
    "4. **Avoiding Wasted Computational Resources:** Training deep learning models can be computationally intensive, especially for large datasets and complex architectures. Early stopping helps avoid unnecessary computational costs associated with training a model for too many epochs without any improvement in validation performance.\n",
    "\n",
    "**Implementation of Early Stopping:**\n",
    "\n",
    "Early stopping requires dividing the dataset into three parts: training, validation, and test sets. The model is trained on the training data while its performance is evaluated on the validation data after each epoch. If the validation performance stops improving or starts degrading for a certain number of epochs (known as the patience parameter), training is halted, and the model's parameters at the point of the best validation performance are used.\n",
    "\n",
    "While early stopping is a powerful regularization technique, it's crucial to strike a balance. Stopping too early might result in an underfit model, while stopping too late allows the model to overfit. Proper validation and tuning of the patience parameter are necessary to ensure the technique is used effectively to prevent overfitting and create models that generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba7bd5",
   "metadata": {},
   "source": [
    "## Ans : 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c8c88",
   "metadata": {},
   "source": [
    "**Batch Normalization:**\n",
    "\n",
    "Batch Normalization (BatchNorm) is a technique used in deep learning to improve the training stability and speed by normalizing the inputs of each layer in a mini-batch. Normalization involves scaling and shifting the inputs to a neural network layer to have a mean of zero and a standard deviation of one. This is done independently for each feature in the input data.\n",
    "\n",
    "During training, BatchNorm normalizes the inputs using the mean and standard deviation of the current mini-batch. It then scales and shifts the normalized values using learnable parameters (gamma and beta) so that the network can learn the optimal scale and shift for each feature. During inference, BatchNorm uses the accumulated mean and standard deviation from the entire training dataset for normalization.\n",
    "\n",
    "**Role as a Form of Regularization:**\n",
    "\n",
    "Batch Normalization acts as a form of regularization for deep learning models due to several reasons:\n",
    "\n",
    "1. **Reduced Internal Covariate Shift:** BatchNorm reduces internal covariate shift, which is the change in the distribution of network activations due to the changing parameters during training. By normalizing inputs within each mini-batch, BatchNorm helps stabilize the training process, enabling the use of higher learning rates and accelerating convergence.\n",
    "\n",
    "2. **Mitigating Vanishing and Exploding Gradients:** Normalizing inputs prevents activations from becoming too large or too small during training. This helps in mitigating issues like vanishing gradients (where gradients become too small) and exploding gradients (where gradients become too large). Stable gradients are essential for training deep networks effectively.\n",
    "\n",
    "3. **Reduced Sensitivity to Weight Initialization:** BatchNorm reduces the sensitivity of the model to the choice of initial weights. Normalizing inputs ensures that the model doesn't depend heavily on the specific initialization of weights, making it easier to train deep networks.\n",
    "\n",
    "4. **Implicit Regularization:** BatchNorm introduces noise in the form of mini-batch statistics. This noise acts as a form of implicit regularization, similar to dropout. By adding noise, BatchNorm prevents the model from fitting the training data too closely, reducing the risk of overfitting.\n",
    "\n",
    "**Preventing Overfitting:**\n",
    "\n",
    "While BatchNorm is not primarily designed as a regularization technique, its stabilization effect during training indirectly helps in preventing overfitting. By enabling faster convergence and reducing the sensitivity of the network to weight initialization, BatchNorm allows for more effective training. When the training process is stable and the model converges faster, it reduces the risk of the model overfitting the training data, especially when the data is noisy or complex.\n",
    "\n",
    "In summary, Batch Normalization helps in preventing overfitting by providing a stable training process, reducing internal covariate shift, mitigating vanishing and exploding gradients, and implicitly adding noise to the training procedure. These effects collectively contribute to a more robust and generalizable model, making BatchNorm an important tool in the regularized training of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd4b34",
   "metadata": {},
   "source": [
    "## Part : 3 \n",
    "\n",
    "## Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3c4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout , Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3bbc73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_val , y_train , y_val = x_train[:55000] , x_train[55000:],y_train[:55000],y_train[55000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfcc921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n",
      "(5000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(55000,)\n",
      "(5000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebcca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model without Dropout\n",
    "model = Sequential([\n",
    "    Flatten(input_shape = (28,28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Model with Dropout\n",
    "model1 = Sequential([\n",
    "    Flatten(input_shape = (28,28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2), \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0730f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy' , optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571f3a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 0.2492 - accuracy: 0.9269 - val_loss: 0.1145 - val_accuracy: 0.9666\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1030 - accuracy: 0.9690 - val_loss: 0.0793 - val_accuracy: 0.9766\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0749 - accuracy: 0.9763 - val_loss: 0.0866 - val_accuracy: 0.9732\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0569 - accuracy: 0.9816 - val_loss: 0.1011 - val_accuracy: 0.9720\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0459 - accuracy: 0.9852 - val_loss: 0.0852 - val_accuracy: 0.9746\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0365 - accuracy: 0.9876 - val_loss: 0.0831 - val_accuracy: 0.9802\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0298 - accuracy: 0.9902 - val_loss: 0.0878 - val_accuracy: 0.9758\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0252 - accuracy: 0.9915 - val_loss: 0.0882 - val_accuracy: 0.9814\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0233 - accuracy: 0.9921 - val_loss: 0.0865 - val_accuracy: 0.9810\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0188 - accuracy: 0.9940 - val_loss: 0.0917 - val_accuracy: 0.9782\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 0.0926 - val_accuracy: 0.9812\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.1001 - val_accuracy: 0.9796\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.1195 - val_accuracy: 0.9784\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.1310 - val_accuracy: 0.9748\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.1203 - val_accuracy: 0.9782\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.1172 - val_accuracy: 0.9806\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.1214 - val_accuracy: 0.9788\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0100 - accuracy: 0.9965 - val_loss: 0.1430 - val_accuracy: 0.9796\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0123 - accuracy: 0.9957 - val_loss: 0.1229 - val_accuracy: 0.9792\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.1331 - val_accuracy: 0.9794\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.1268 - val_accuracy: 0.9808\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0096 - accuracy: 0.9968 - val_loss: 0.1331 - val_accuracy: 0.9796\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.1398 - val_accuracy: 0.9806\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.1594 - val_accuracy: 0.9788\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.1616 - val_accuracy: 0.9800\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0103 - accuracy: 0.9971 - val_loss: 0.1698 - val_accuracy: 0.9782\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.1741 - val_accuracy: 0.9772\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.1855 - val_accuracy: 0.9798\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.1565 - val_accuracy: 0.9804\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.1795 - val_accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train , y_train , validation_data=(x_val , y_val), epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f487c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.249202</td>\n",
       "      <td>0.926909</td>\n",
       "      <td>0.114455</td>\n",
       "      <td>0.9666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.969018</td>\n",
       "      <td>0.079314</td>\n",
       "      <td>0.9766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074884</td>\n",
       "      <td>0.976327</td>\n",
       "      <td>0.086636</td>\n",
       "      <td>0.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056902</td>\n",
       "      <td>0.981618</td>\n",
       "      <td>0.101055</td>\n",
       "      <td>0.9720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045860</td>\n",
       "      <td>0.985236</td>\n",
       "      <td>0.085237</td>\n",
       "      <td>0.9746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.249202  0.926909  0.114455        0.9666\n",
       "1  0.103000  0.969018  0.079314        0.9766\n",
       "2  0.074884  0.976327  0.086636        0.9732\n",
       "3  0.056902  0.981618  0.101055        0.9720\n",
       "4  0.045860  0.985236  0.085237        0.9746"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(history.history)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae21fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5a3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss = 'sparse_categorical_crossentropy' , optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316a1cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 0.3504 - accuracy: 0.8959 - val_loss: 0.1054 - val_accuracy: 0.9700\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1678 - accuracy: 0.9503 - val_loss: 0.0847 - val_accuracy: 0.9746\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1276 - accuracy: 0.9626 - val_loss: 0.0766 - val_accuracy: 0.9778\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1108 - accuracy: 0.9661 - val_loss: 0.0736 - val_accuracy: 0.9792\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0974 - accuracy: 0.9701 - val_loss: 0.0740 - val_accuracy: 0.9782\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0860 - accuracy: 0.9739 - val_loss: 0.0697 - val_accuracy: 0.9828\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0791 - accuracy: 0.9756 - val_loss: 0.0895 - val_accuracy: 0.9798\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0744 - accuracy: 0.9769 - val_loss: 0.0708 - val_accuracy: 0.9804\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0680 - accuracy: 0.9781 - val_loss: 0.0790 - val_accuracy: 0.9798\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0674 - accuracy: 0.9785 - val_loss: 0.0732 - val_accuracy: 0.9816\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0596 - accuracy: 0.9811 - val_loss: 0.0709 - val_accuracy: 0.9824\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0581 - accuracy: 0.9813 - val_loss: 0.0731 - val_accuracy: 0.9802\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0549 - accuracy: 0.9828 - val_loss: 0.0713 - val_accuracy: 0.9818\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0522 - accuracy: 0.9826 - val_loss: 0.0774 - val_accuracy: 0.9834\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0514 - accuracy: 0.9831 - val_loss: 0.0728 - val_accuracy: 0.9826\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0504 - accuracy: 0.9840 - val_loss: 0.0748 - val_accuracy: 0.9826\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0481 - accuracy: 0.9840 - val_loss: 0.0795 - val_accuracy: 0.9814\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0482 - accuracy: 0.9846 - val_loss: 0.0706 - val_accuracy: 0.9830\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0461 - accuracy: 0.9854 - val_loss: 0.0732 - val_accuracy: 0.9838\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0439 - accuracy: 0.9861 - val_loss: 0.0844 - val_accuracy: 0.9820\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0414 - accuracy: 0.9865 - val_loss: 0.0760 - val_accuracy: 0.9836\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0437 - accuracy: 0.9863 - val_loss: 0.0780 - val_accuracy: 0.9824\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0407 - accuracy: 0.9865 - val_loss: 0.0724 - val_accuracy: 0.9848\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0384 - accuracy: 0.9873 - val_loss: 0.0852 - val_accuracy: 0.9826\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0398 - accuracy: 0.9869 - val_loss: 0.0849 - val_accuracy: 0.9818\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0370 - accuracy: 0.9878 - val_loss: 0.0843 - val_accuracy: 0.9816\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0391 - accuracy: 0.9874 - val_loss: 0.0802 - val_accuracy: 0.9834\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0354 - accuracy: 0.9883 - val_loss: 0.0862 - val_accuracy: 0.9830\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0358 - accuracy: 0.9884 - val_loss: 0.0806 - val_accuracy: 0.9826\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0885 - val_accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(x_train , y_train , validation_data=(x_val , y_val), epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a74a5992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350383</td>\n",
       "      <td>0.895909</td>\n",
       "      <td>0.105445</td>\n",
       "      <td>0.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.167782</td>\n",
       "      <td>0.950273</td>\n",
       "      <td>0.084732</td>\n",
       "      <td>0.9746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.127644</td>\n",
       "      <td>0.962618</td>\n",
       "      <td>0.076625</td>\n",
       "      <td>0.9778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110771</td>\n",
       "      <td>0.966127</td>\n",
       "      <td>0.073628</td>\n",
       "      <td>0.9792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.097359</td>\n",
       "      <td>0.970091</td>\n",
       "      <td>0.074038</td>\n",
       "      <td>0.9782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.350383  0.895909  0.105445        0.9700\n",
       "1  0.167782  0.950273  0.084732        0.9746\n",
       "2  0.127644  0.962618  0.076625        0.9778\n",
       "3  0.110771  0.966127  0.073628        0.9792\n",
       "4  0.097359  0.970091  0.074038        0.9782"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.DataFrame(history1.history)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed27003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bed64",
   "metadata": {},
   "source": [
    "## Ans : 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7439e",
   "metadata": {},
   "source": [
    "**Considerations and Tradeoffs when Choosing Regularization Techniques:**\n",
    "\n",
    "Dataset Size: Regularization techniques like Dropout are particularly useful when you have a limited amount of training data. If your dataset is large and diverse, you might not need aggressive regularization.\n",
    "\n",
    "Model Complexity: If your model is complex with many layers and parameters, regularization becomes crucial to prevent overfitting. Simpler models might need less aggressive regularization.\n",
    "\n",
    "Computational Resources: Regularization techniques like Dropout introduce noise and require additional computations during training. Consider your computational resources when choosing regularization methods. For large-scale models, these computations might impact training time significantly.\n",
    "\n",
    "Domain Knowledge: Understand your data and the problem domain. Some regularization techniques might align better with the inherent properties of the data. For example, spatial dropout might be more suitable for image data.\n",
    "\n",
    "Experimentation: It's often a good practice to experiment with different regularization techniques (e.g., L1, L2, Dropout) and find the one that provides the best balance between training performance and generalization on your specific dataset.\n",
    "\n",
    "Validation Performance: Regularization techniques should be validated on a separate validation dataset (or through cross-validation) to ensure they improve generalization without sacrificing too much of the training performance.\n",
    "\n",
    "Choosing the appropriate regularization technique involves understanding the problem context, the available data, and experimenting with different methods to strike the right balance between preventing overfitting and maintaining good model performance. Regularization should be tailored to the specific characteristics of the data and the complexity of the model being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a53b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
