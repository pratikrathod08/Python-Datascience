{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db70b65d",
   "metadata": {},
   "source": [
    "## Part : 1 \n",
    "\n",
    "## Ans : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26acda3c",
   "metadata": {},
   "source": [
    "Optimization algorithms in artificial neural networks play a fundamental role in the training process. The primary objective of training a neural network is to minimize a specific cost or loss function. This function measures the disparity between the predicted outputs generated by the neural network and the actual targets (ground truth) in the training data. The optimization algorithm's purpose is to adjust the parameters (weights and biases) of the neural network to minimize this loss function. \n",
    "\n",
    "Here's why optimization algorithms are necessary:\n",
    "\n",
    "1. **Minimization of Loss Function:** Neural networks consist of numerous parameters, and finding the optimal combination of these parameters that results in the lowest possible loss is a complex, high-dimensional optimization problem. Optimization algorithms are designed to navigate this vast parameter space efficiently and find the set of parameters that correspond to the minimum value of the loss function.\n",
    "\n",
    "2. **Learning from Data:** Neural networks learn from data by adjusting their parameters based on the errors made during predictions. Optimization algorithms facilitate this learning process by iteratively updating the parameters, reducing the prediction errors, and improving the network's performance on the given task.\n",
    "\n",
    "3. **Generalization:** The goal of training a neural network is not just to memorize the training data but to generalize well to unseen, new data. Optimization algorithms help in finding parameter values that enable the network to generalize its learning from the training data to make accurate predictions on unseen data.\n",
    "\n",
    "4. **Efficiency:** Optimization algorithms are designed to make the training process computationally feasible and efficient. Training neural networks without optimization algorithms would require exhaustive search, which is practically impossible for large and complex networks due to the enormous number of possible parameter combinations.\n",
    "\n",
    "In summary, optimization algorithms are necessary in artificial neural networks because they enable the networks to learn from data, minimize prediction errors, generalize to new data, and do so efficiently by navigating the vast and complex parameter space, ultimately leading to improved model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6292507",
   "metadata": {},
   "source": [
    "## Ans : 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68021d",
   "metadata": {},
   "source": [
    "**Gradient Descent and its Variants:**\n",
    "\n",
    "**Gradient Descent:**\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize a function, such as the loss function in neural network training. The basic idea is to update the parameters of the model in the direction opposite to the gradient of the function. Mathematically, the update rule for a parameter \\(w\\) can be expressed as:\n",
    "\n",
    "\\[w = w - \\eta \\times \\nabla F(w)\\]\n",
    "\n",
    "Where:\n",
    "- \\(w\\) is the parameter being optimized.\n",
    "- \\(\\eta\\) (eta) is the learning rate, which determines the size of the steps taken during optimization.\n",
    "- \\(\\nabla F(w)\\) represents the gradient of the function \\(F(w)\\) with respect to the parameter \\(w\\).\n",
    "\n",
    "Gradient Descent methods differ in how they compute the gradient and update the parameters.\n",
    "\n",
    "**Variants of Gradient Descent:**\n",
    "\n",
    "1. **Batch Gradient Descent:**\n",
    "   - **Computation:** Computes the gradient of the entire dataset.\n",
    "   - **Update:** Performs a parameter update after processing the entire dataset.\n",
    "   - **Tradeoff:** Accurate gradient estimation but can be computationally expensive, especially for large datasets due to memory requirements.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD):**\n",
    "   - **Computation:** Computes the gradient and performs a parameter update for each training example.\n",
    "   - **Update:** Updates the parameters frequently, making it noisy but faster.\n",
    "   - **Tradeoff:** Faster convergence due to frequent updates but can be noisy and might oscillate around the minimum.\n",
    "\n",
    "3. **Mini-batch Gradient Descent:**\n",
    "   - **Computation:** Computes the gradient and updates parameters using a random subset of the data (mini-batch).\n",
    "   - **Update:** Strikes a balance between accuracy and computational efficiency by using a subset of data.\n",
    "   - **Tradeoff:** Efficient use of memory and computation, balancing accuracy and speed.\n",
    "\n",
    "**Differences and Tradeoffs:**\n",
    "\n",
    "- **Convergence Speed:** \n",
    "  - **Batch Gradient Descent:** Slower due to infrequent updates, but the updates are more accurate.\n",
    "  - **SGD:** Faster due to frequent updates, especially for large datasets, but can be noisy.\n",
    "  - **Mini-batch Gradient Descent:** Balance between batch and SGD, offering moderate speed and accuracy.\n",
    "\n",
    "- **Memory Requirements:** \n",
    "  - **Batch Gradient Descent:** Requires memory to store the entire dataset, can be inefficient for large datasets.\n",
    "  - **SGD:** Requires memory for only one training example at a time, making it memory efficient.\n",
    "  - **Mini-batch Gradient Descent:** Consumes memory for a subset of data, making it suitable for moderately sized datasets.\n",
    "\n",
    "In summary, the choice of gradient descent variant depends on the dataset size, computational resources, and desired convergence speed. Batch Gradient Descent provides accurate gradients but can be slow and memory-intensive. SGD and Mini-batch Gradient Descent offer faster convergence with less memory consumption, striking a balance between accuracy and speed. The tradeoffs involve accuracy, speed, and memory efficiency, and the choice of the variant is crucial in optimizing neural networks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f50b2b",
   "metadata": {},
   "source": [
    "## Ans : 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39598e8",
   "metadata": {},
   "source": [
    "Traditional gradient descent optimization methods, including Batch Gradient Descent, Stochastic Gradient Descent (SGD), and Mini-batch Gradient Descent, face several challenges that can hinder their efficiency and effectiveness in training neural networks. Here are the challenges and how modern optimizers address them:\n",
    "\n",
    "**1. Slow Convergence:**\n",
    "Traditional gradient descent methods can converge to the minimum very slowly, especially in high-dimensional spaces. This slow convergence can significantly prolong the training process, making it impractical for large and complex neural networks.\n",
    "\n",
    "**Modern Optimizer Solution:** Modern optimizers, such as Adam, RMSProp, and Adagrad, incorporate adaptive learning rates for each parameter. Adaptive learning rate algorithms adjust the step size based on the historical gradients, allowing for faster convergence. By dynamically scaling the learning rates, these optimizers can speed up the convergence process, ensuring quicker training of neural networks.\n",
    "\n",
    "**2. Local Minima:**\n",
    "Traditional gradient descent methods can get stuck in local minima, which are suboptimal points in the loss function landscape. Getting trapped in local minima prevents the model from finding the global minimum, leading to less accurate results.\n",
    "\n",
    "**Modern Optimizer Solution:** Modern optimizers use techniques like momentum and adaptive learning rates to escape local minima. By incorporating momentum, the optimizer can continue moving in a certain direction even if the gradient momentarily points in a different direction. Adaptive learning rate methods adjust the step sizes, enabling the optimizer to navigate the loss landscape more effectively, avoiding getting stuck in local minima and finding better optima, including the global minimum.\n",
    "\n",
    "**3. Plateaus and Saddle Points:**\n",
    "Traditional gradient descent methods can slow down or stall in flat regions of the loss function (plateaus) or at saddle points, where the gradient is zero but isn't a minimum.\n",
    "\n",
    "**Modern Optimizer Solution:** Modern optimizers handle plateaus and saddle points better due to their adaptive learning rate mechanisms. When the optimizer encounters flat regions or saddle points, the adaptive learning rate adjusts the step size, preventing the optimization process from slowing down significantly. This adaptability allows modern optimizers to navigate through these challenging regions more effectively.\n",
    "\n",
    "**4. Poor Generalization:**\n",
    "Traditional gradient descent methods may lead to overfitting, where the model performs well on the training data but poorly on unseen data, indicating poor generalization.\n",
    "\n",
    "**Modern Optimizer Solution:** While not directly addressing generalization, modern optimizers indirectly contribute to better generalization by allowing the model to converge more quickly and efficiently. Faster training often results in models that generalize better, especially when combined with techniques like early stopping and regularization methods.\n",
    "\n",
    "In summary, modern optimizers address the challenges associated with traditional gradient descent methods by incorporating adaptive learning rates, momentum, and other techniques. These adaptations enable the optimizers to converge faster, escape local minima, handle plateaus and saddle points, and ultimately improve the training efficiency and effectiveness of neural networks, leading to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6e1b8",
   "metadata": {},
   "source": [
    "## Ans : 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc4f1a",
   "metadata": {},
   "source": [
    "**Momentum in Optimization Algorithms:**\n",
    "\n",
    "Momentum is a technique used in optimization algorithms to accelerate the convergence process, especially when dealing with flat or curved surfaces in the loss landscape. Instead of just considering the current gradient to determine the update direction, momentum methods also take into account the accumulated past gradients. This accumulated gradient acts as a velocity term, influencing the direction and speed of the parameter updates.\n",
    "\n",
    "Momentum helps the optimization algorithm to:\n",
    "\n",
    "- **Continue Moving:** Even when the gradient momentarily points in a different direction, momentum allows the optimization process to keep moving in the previous direction. This helps the algorithm to avoid getting stuck in local minima or slow convergence in flat regions.\n",
    "\n",
    "- **Smooth Optimization:** By averaging out the gradients, momentum smoothens the optimization process, allowing for more stable and consistent updates. This can prevent oscillations and make convergence more predictable.\n",
    "\n",
    "- **Escape Local Minima:** Momentum assists in escaping local minima by providing the necessary inertia to overcome small barriers and continue searching for the global minimum.\n",
    "\n",
    "**Learning Rate in Optimization Algorithms:**\n",
    "\n",
    "Learning rate is a hyperparameter that controls the size of the steps taken during optimization. A high learning rate allows the algorithm to converge quickly, but it might overshoot the optimal solution. On the other hand, a low learning rate results in slower convergence, but it can help the algorithm converge to a more precise minimum.\n",
    "\n",
    "- **Impact on Convergence:**\n",
    "  - **High Learning Rate:** Fast convergence, but may overshoot the optimal solution and oscillate around it.\n",
    "  - **Low Learning Rate:** Slow convergence, but a higher chance of converging to a more accurate minimum as it explores the space more finely.\n",
    "\n",
    "- **Impact on Model Performance:**\n",
    "  - **Learning rate that is too high:** Might cause the algorithm to miss the optimal solution, leading to poor model performance and overshooting the minimum.\n",
    "  - **Learning rate that is too low:** May get stuck in local minima or take an excessively long time to converge, impacting model performance and training efficiency.\n",
    "\n",
    "**Interaction Between Momentum and Learning Rate:**\n",
    "\n",
    "- **Synergy:** Momentum and learning rate often work together synergistically. Momentum helps the algorithm move faster in directions of consistent improvement, while the learning rate fine-tunes the step size, ensuring that these movements are not too drastic or too small.\n",
    "\n",
    "- **Tuning:** Proper tuning of both momentum and learning rate is essential. If momentum is too high, it might cause overshooting, especially when combined with a high learning rate. Similarly, a low momentum might slow down convergence even with an optimal learning rate.\n",
    "\n",
    "In summary, momentum and learning rate are crucial hyperparameters in optimization algorithms. Momentum provides inertia to overcome obstacles in the loss landscape, while the learning rate determines the step size during optimization. Properly balancing these factors is vital for achieving faster convergence and optimal model performance. Experimentation and tuning are often necessary to find the right combination of momentum and learning rate for a specific optimization problem and neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15986f63",
   "metadata": {},
   "source": [
    "## Part : 2 \n",
    "\n",
    "## Ans : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff616612",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent (SGD):**\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a variant of the gradient descent optimization algorithm. While traditional gradient descent methods compute gradients using the entire dataset (Batch Gradient Descent) or a subset (Mini-batch Gradient Descent) before updating the model parameters, SGD calculates the gradient and updates the parameters for each individual training example.\n",
    "\n",
    "**Advantages of SGD Compared to Traditional Gradient Descent:**\n",
    "\n",
    "1. **Faster Convergence:** Because SGD updates the parameters after processing each training example, it converges faster in many cases. This frequent updating allows the model to adjust quickly to the data, especially in high-dimensional spaces.\n",
    "\n",
    "2. **Avoiding Local Minima:** Due to its stochastic nature, SGD can escape local minima and saddle points more easily than batch gradient descent. The noisy updates allow SGD to jump out of suboptimal points, enabling it to find better solutions.\n",
    "\n",
    "3. **Memory Efficiency:** SGD processes one training example at a time, making it memory efficient, particularly for large datasets. Traditional gradient descent methods require storing the entire dataset in memory, which can be challenging for big datasets.\n",
    "\n",
    "4. **Exploration of the Solution Space:** The noise introduced by processing individual examples allows SGD to explore a broader area of the solution space. This exploration can lead to finding better minima and improves the model's ability to generalize to unseen data.\n",
    "\n",
    "**Limitations and Suitable Scenarios for SGD:**\n",
    "\n",
    "1. **Noisy Updates:** The noisy updates in SGD can lead to oscillations around the optimal solution, making the convergence path erratic. This noise can cause the optimization process to overshoot the minimum and affect the stability of the training process.\n",
    "\n",
    "2. **Slower Convergence for Noisy Data:** If the training data is noisy, the noisy updates from individual examples can prevent the model from converging to a good solution. In such cases, methods like Mini-batch Gradient Descent might be more suitable.\n",
    "\n",
    "3. **Learning Rate Tuning:** SGD requires careful tuning of the learning rate. Too high a learning rate can cause overshooting, while too low a learning rate can result in slow convergence. Finding the right learning rate is crucial for the algorithm's performance.\n",
    "\n",
    "**Suitable Scenarios for SGD:**\n",
    "\n",
    "1. **Large Datasets:** SGD is particularly useful for training on large datasets where processing the entire dataset at once is computationally infeasible due to memory constraints.\n",
    "\n",
    "2. **Online Learning:** In scenarios where new data points are continuously available, SGD is well-suited. It can be updated online as new data arrives, ensuring that the model adapts to the changing data distribution.\n",
    "\n",
    "3. **Non-Convex Optimization:** For non-convex optimization problems, where the loss landscape has multiple minima, SGD can explore different regions and potentially find a better solution than batch gradient descent methods.\n",
    "\n",
    "In summary, SGD is advantageous for its fast convergence, memory efficiency, and ability to escape local minima, especially in large-scale and dynamic learning scenarios. However, its noisy updates and the need for careful learning rate tuning make it important to consider the specific characteristics of the problem and dataset when choosing the optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad038b82",
   "metadata": {},
   "source": [
    "## Ans: 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aacfd4",
   "metadata": {},
   "source": [
    "**Adam Optimizer:**\n",
    "\n",
    "Adam (short for Adaptive Moment Estimation) is a popular optimization algorithm that combines the advantages of both momentum and adaptive learning rate methods. It maintains two moving averages: the first moment (mean) of the gradients (similar to momentum) and the second moment (uncentered variance), which is akin to adapting the learning rates for each parameter.\n",
    "\n",
    "**How Adam Combines Momentum and Adaptive Learning Rates:**\n",
    "\n",
    "1. **Momentum Component:** Adam uses the moving average of gradients (first moment), similar to how momentum helps the optimization process move more swiftly in the relevant directions.\n",
    "\n",
    "2. **Adaptive Learning Rate Component:** Adam also keeps track of the uncentered variance (second moment) of the gradients. By dividing the first moment by the square root of the second moment, Adam effectively adapts the learning rate for each parameter. This adaptive learning rate mechanism ensures that each parameter has a unique learning rate based on the historical gradients.\n",
    "\n",
    "**Benefits of Adam Optimizer:**\n",
    "\n",
    "1. **Adaptability:** Adam adapts the learning rates for each parameter individually, allowing it to handle sparse gradients and varying importance of parameters effectively. It can converge quickly even when dealing with features that occur infrequently in the dataset.\n",
    "\n",
    "2. **Efficiency:** Adam often converges faster than traditional gradient descent methods due to its adaptive learning rate and momentum-like behavior. It combines the advantages of both techniques, leading to efficient optimization.\n",
    "\n",
    "3. **Robustness:** Adam is less sensitive to hyperparameter choices, making it a practical choice for many applications. It typically performs well with default hyperparameters in a wide range of scenarios.\n",
    "\n",
    "4. **Handling Noisy or Sparse Data:** Due to its adaptive learning rate, Adam is suitable for datasets with noisy or sparse gradients. It can adjust the learning rates based on the gradients' variance, ensuring stable convergence.\n",
    "\n",
    "**Potential Drawbacks of Adam Optimizer:**\n",
    "\n",
    "1. **Memory Requirements:** Adam maintains additional moving average parameters for each trainable parameter, which can increase memory usage, especially for large models with many parameters. This can be a concern for memory-limited environments.\n",
    "\n",
    "2. **Overfitting:** In some cases, Adam might adapt too quickly to the noisy gradients, potentially leading to overfitting, especially in situations where the noise in the data is significant.\n",
    "\n",
    "3. **Lack of Theoretical Understanding:** While Adam has shown empirical success in various applications, its complex behavior and lack of a clear theoretical understanding can make it challenging to predict its behavior in specific cases.\n",
    "\n",
    "In summary, Adam optimizer combines momentum and adaptive learning rates, providing adaptability, efficiency, and robustness. However, practitioners should be mindful of its memory requirements and potential overfitting, especially in noisy data scenarios. As with any optimization algorithm, it's essential to experiment and validate its performance on the specific problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691ea17",
   "metadata": {},
   "source": [
    "## Ans : 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b5084",
   "metadata": {},
   "source": [
    "**RMSprop Optimizer:**\n",
    "\n",
    "Root Mean Square Propagation (RMSprop) is an optimization algorithm that tackles the challenge of adaptive learning rates. It modifies the learning rates for each parameter during training, ensuring that the optimization process is efficient and effective.\n",
    "\n",
    "**How RMSprop Works:**\n",
    "\n",
    "RMSprop maintains a moving average of the squared gradients for each parameter. Instead of adapting the learning rate based on the second moment of gradients (as in Adam), RMSprop divides the current gradient by the square root of the moving average of past squared gradients. This adaptive scaling allows RMSprop to handle varying gradients effectively, adjusting the learning rates according to the historical gradient information.\n",
    "\n",
    "**Comparison with Adam:**\n",
    "\n",
    "1. **RMSprop vs. Adaptive Learning Rates:**\n",
    "   - **RMSprop:** Adapts learning rates using the moving average of squared gradients, providing a stable and adaptive approach.\n",
    "   - **Adam:** Combines adaptive learning rates with momentum, incorporating both first and second moments of gradients for adaptability.\n",
    "\n",
    "2. **Strengths:**\n",
    "   - **RMSprop:**\n",
    "     - Simplicity: RMSprop is relatively simpler than Adam, making it computationally more efficient.\n",
    "     - Stable Performance: RMSprop often provides stable performance across a wide range of applications and datasets.\n",
    "   - **Adam:**\n",
    "     - Efficiency: Adam combines adaptive learning rates with momentum, making it efficient for convergence, especially in high-dimensional spaces.\n",
    "     - Robustness: Adam's combination of techniques often results in robust convergence and generalization.\n",
    "\n",
    "3. **Weaknesses:**\n",
    "   - **RMSprop:**\n",
    "     - Lack of Momentum: RMSprop does not include a momentum term, which can sometimes slow down convergence in certain scenarios, especially when dealing with noisy gradients.\n",
    "   - **Adam:**\n",
    "     - Complexity: Adam's additional parameters and complexity might require more careful tuning than RMSprop, and it can sometimes overfit on small datasets or noisy gradients.\n",
    "\n",
    "**Relative Strengths and Weaknesses:**\n",
    "\n",
    "- **RMSprop:**\n",
    "  - **Strengths:** Simplicity, stability, and computationally efficient for many applications.\n",
    "  - **Weaknesses:** Lack of momentum might affect convergence speed in some cases.\n",
    "\n",
    "- **Adam:**\n",
    "  - **Strengths:** Efficiency, robustness, and adaptability to various scenarios.\n",
    "  - **Weaknesses:** Complexity can lead to overfitting in certain situations, and tuning hyperparameters can be crucial.\n",
    "\n",
    "**Choosing Between RMSprop and Adam:**\n",
    "\n",
    "- **RMSprop:** Choose RMSprop when you prefer a simpler optimizer that provides stable performance across various scenarios, especially when computational resources are limited.\n",
    "\n",
    "- **Adam:** Choose Adam when you need an efficient and robust optimizer, especially in high-dimensional spaces, and you are willing to experiment and fine-tune the hyperparameters for optimal performance.\n",
    "\n",
    "Ultimately, the choice between RMSprop and Adam depends on the specific problem, the complexity of the neural network, the available computational resources, and the need for adaptability and stability in the optimization process. Experimentation and empirical validation are key to determining which optimizer performs best for a particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6eb8b",
   "metadata": {},
   "source": [
    "## Part : 3 \n",
    "\n",
    "## Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9fb471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5715 - accuracy: 0.8553 - val_loss: 0.2962 - val_accuracy: 0.9147\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2793 - accuracy: 0.9204 - val_loss: 0.2402 - val_accuracy: 0.9310\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2283 - accuracy: 0.9354 - val_loss: 0.2090 - val_accuracy: 0.9378\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1940 - accuracy: 0.9450 - val_loss: 0.1778 - val_accuracy: 0.9478\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1688 - accuracy: 0.9516 - val_loss: 0.1574 - val_accuracy: 0.9541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1965f7be220>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(300 , activation ='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64a95920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2050 - accuracy: 0.9383 - val_loss: 0.1128 - val_accuracy: 0.9672\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0842 - accuracy: 0.9739 - val_loss: 0.0829 - val_accuracy: 0.9730\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0573 - accuracy: 0.9815 - val_loss: 0.0857 - val_accuracy: 0.9715\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0422 - accuracy: 0.9861 - val_loss: 0.0878 - val_accuracy: 0.9737\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0337 - accuracy: 0.9889 - val_loss: 0.0712 - val_accuracy: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1965f0125b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(300 , activation ='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3b6cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2076 - accuracy: 0.9374 - val_loss: 0.1369 - val_accuracy: 0.9573\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0912 - accuracy: 0.9730 - val_loss: 0.0886 - val_accuracy: 0.9754\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0655 - accuracy: 0.9805 - val_loss: 0.0784 - val_accuracy: 0.9777\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0506 - accuracy: 0.9855 - val_loss: 0.0883 - val_accuracy: 0.9771\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0404 - accuracy: 0.9882 - val_loss: 0.0845 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1967b77cfa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(300 , activation ='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='RMSprop', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df05aa5",
   "metadata": {},
   "source": [
    "## Ans : 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cec2e",
   "metadata": {},
   "source": [
    "### Considerations and Tradeoffs:\n",
    "\n",
    "1. **Convergence Speed:**\n",
    "   - **SGD:** Might converge slower due to noisy updates, especially on large datasets.\n",
    "   - **Adam and RMSprop:** Generally converge faster due to adaptive learning rates.\n",
    "\n",
    "2. **Stability:**\n",
    "   - **SGD:** Prone to oscillations due to noisy updates, might get stuck in local minima.\n",
    "   - **Adam and RMSprop:** More stable due to adaptive learning rates, especially in high-dimensional spaces.\n",
    "\n",
    "3. **Generalization Performance:**\n",
    "   - **SGD:** May escape local minima, potentially leading to better generalization, especially for non-convex loss functions.\n",
    "   - **Adam and RMSprop:** Faster convergence might lead to more accurate solutions, but there's a risk of overfitting, especially if the dataset is small or noisy.\n",
    "\n",
    "4. **Hyperparameter Sensitivity:**\n",
    "   - **SGD:** Requires tuning of learning rate and momentum, which can be sensitive to the choice of values.\n",
    "   - **Adam and RMSprop:** More robust to hyperparameter choices, making them easier to use out of the box.\n",
    "\n",
    "5. **Memory Usage:**\n",
    "   - **SGD:** Requires less memory as it processes one example at a time.\n",
    "   - **Adam and RMSprop:** Require more memory due to maintaining additional moving averages, especially for large models.\n",
    "\n",
    "6. **Computational Efficiency:**\n",
    "   - **SGD:** Computationally efficient, especially with mini-batch training.\n",
    "   - **Adam and RMSprop:** Slightly less computationally efficient due to additional computations for adaptive learning rates.\n",
    "\n",
    "When choosing an optimizer:\n",
    "\n",
    "- **For Stable Convergence:** If stability is a concern, especially in noisy or complex loss landscapes, Adam or RMSprop might be preferred due to their adaptive learning rates and stable convergence behavior.\n",
    "\n",
    "- **For Generalization:** If the goal is to find a more diverse set of solutions to potentially escape local minima and improve generalization, SGD might be preferred, especially for non-convex problems.\n",
    "\n",
    "- **For Large Datasets:** Adam and RMSprop, with their adaptive learning rates, often perform well on large datasets and high-dimensional spaces.\n",
    "\n",
    "- **For Robustness:** Adam and RMSprop are often more robust to hyperparameter choices, making them suitable choices if you don't want to spend extensive time tuning hyperparameters.\n",
    "\n",
    "Experimentation and validation on the specific task at hand are essential to choose the most appropriate optimizer for a given neural network architecture and task in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219878d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
