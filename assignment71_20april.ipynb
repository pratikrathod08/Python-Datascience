{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ba2e22",
   "metadata": {},
   "source": [
    "## Ans : 1\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric supervised learning algorithm used for both classification and regression tasks. It makes predictions by identifying the K nearest data points in the training set to a given test data point, based on a distance metric (e.g., Euclidean or Manhattan distance). The predicted output is determined by a majority vote (for classification) or an average (for regression) of the labels/values of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19600b73",
   "metadata": {},
   "source": [
    "## Ans : 2 \n",
    "\n",
    "The choice of the value of K in KNN is crucial and can significantly impact the performance of the algorithm. A smaller value of K makes the model more sensitive to noise and outliers, while a larger value of K makes the decision boundaries smoother but can lead to the loss of local patterns.\n",
    "\n",
    "The value of K can be chosen using various methods, including:\n",
    "\n",
    "Rule of thumb: K = sqrt(N), where N is the number of samples in the training set.\n",
    "Cross-validation: Perform k-fold cross-validation on the training set and select the value of K that gives the best performance metric (e.g., accuracy, mean squared error).\n",
    "Grid search: Evaluate the model's performance for different values of K and select the one that yields the best results.\n",
    "Domain knowledge: Based on the nature of the problem and understanding of the data, choose a value of K that is expected to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23600c",
   "metadata": {},
   "source": [
    "## Ans : 3\n",
    "\n",
    "The difference between KNN classifier and KNN regressor lies in their output.\n",
    "\n",
    "KNN Classifier:\n",
    "In KNN classification, the output is a class label or category. It assigns the class label based on the majority vote of the K nearest neighbors. For example, if K = 5 and three neighbors belong to class A while two neighbors belong to class B, the predicted label for the test instance would be class A.\n",
    "\n",
    "KNN Regressor:\n",
    "In KNN regression, the output is a continuous numerical value. It calculates the average (or weighted average) of the values of the K nearest neighbors and uses that as the predicted value. For example, if K = 3 and the values of the three nearest neighbors are 4, 5, and 6, the predicted value for the test instance would be (4 + 5 + 6) / 3 = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ac45a",
   "metadata": {},
   "source": [
    "## Ans : 4\n",
    "\n",
    "The performance of the KNN algorithm can be measured using various evaluation metrics, depending on the task at hand:\n",
    "\n",
    "For classification tasks:\n",
    "\n",
    "Accuracy: The ratio of correctly classified instances to the total number of instances.\n",
    "Precision: The ratio of true positives to the sum of true positives and false positives, measuring the classifier's ability to correctly identify positive instances.\n",
    "Recall: The ratio of true positives to the sum of true positives and false negatives, measuring the classifier's ability to find all positive instances.\n",
    "F1 score: The harmonic mean of precision and recall, providing a balanced measure between the two.\n",
    "For regression tasks:\n",
    "\n",
    "Mean Absolute Error (MAE): The average of the absolute differences between the predicted and actual values.\n",
    "Mean Squared Error (MSE): The average of the squared differences between the predicted and actual values.\n",
    "Root Mean Squared Error (RMSE): The square root of MSE, providing a more interpretable measure in the original scale of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512042cc",
   "metadata": {},
   "source": [
    "## Ans : 5\n",
    "\n",
    "The curse of dimensionality refers to the problem that occurs when the number of features (dimensions) in a dataset increases. In KNN, the curse of dimensionality can impact the performance of the algorithm in several ways:\n",
    "\n",
    "Increased computational complexity: As the number of dimensions increases, the search space grows exponentially, making it computationally expensive to find the nearest neighbors.\n",
    "\n",
    "Sparsity of data: In high-dimensional spaces, data points tend to become more sparse, making it difficult to find similar instances. The increased sparsity leads to a higher risk of misclassification or regression errors.\n",
    "\n",
    "Degraded effectiveness of distance measures: In high-dimensional spaces, the concept of distance becomes less meaningful. The distances between points become more evenly distributed, and the notion of proximity loses its significance.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN, techniques such as dimensionality reduction (e.g., PCA) or feature selection can be applied to reduce the number of dimensions and improve the algorithm's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f754a",
   "metadata": {},
   "source": [
    "## Ans : 6\n",
    "\n",
    "KNN can handle missing values by imputing them with plausible values based on the neighboring data points. The following steps can be followed:\n",
    "\n",
    "Identify missing values: Identify the features with missing values in the dataset.\n",
    "\n",
    "Distance computation: Compute the distance between each data point with missing values and the other data points.\n",
    "\n",
    "Select neighbors: Select the K nearest neighbors based on the computed distances.\n",
    "\n",
    "Imputation: For each missing value, impute it with the average (for regression) or the mode (for classification) of the corresponding feature values among the K nearest neighbors.\n",
    "\n",
    "Repeat: Repeat the process for all missing values in the dataset.\n",
    "\n",
    "It's important to note that the choice of distance metric can impact the imputation process, and imputing missing values with neighborhood statistics assumes that similar instances have similar values for the missing feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0cc9bb",
   "metadata": {},
   "source": [
    "## Ans : 7\n",
    "\n",
    "The choice between the KNN classifier and regressor depends on the nature of the problem and the type of output required:\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "Suitable for categorical/classification problems where the goal is to predict discrete class labels.\n",
    "Predicts the most frequent class label among the K nearest neighbors.\n",
    "Evaluation metrics include accuracy, precision, recall, and F1 score.\n",
    "Examples: Predicting whether an email is spam or not, classifying images into different categories.\n",
    "KNN Regressor:\n",
    "\n",
    "Suitable for continuous/numerical prediction problems where the goal is to estimate a specific value.\n",
    "Predicts the average (or weighted average) of the values among the K nearest neighbors.\n",
    "Evaluation metrics include mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE).\n",
    "Examples: Predicting housing prices based on features, estimating the sales volume of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db143a94",
   "metadata": {},
   "source": [
    "## Ans : 8\n",
    "\n",
    "Strengths of KNN:\n",
    "\n",
    "Simple and intuitive algorithm.\n",
    "Can handle both classification and regression tasks.\n",
    "No assumptions about the underlying data distribution.\n",
    "Robust to noisy data and outliers.\n",
    "Weaknesses of KNN:\n",
    "\n",
    "Computationally expensive for large datasets.\n",
    "Sensitive to the choice of K and the distance metric.\n",
    "Performance can degrade in high-dimensional spaces (curse of dimensionality).\n",
    "Requires feature scaling for distance-based calculations.\n",
    "To address these weaknesses:\n",
    "\n",
    "Use efficient data structures (e.g., KD-trees, Ball trees) to speed up the search for nearest neighbors.\n",
    "Perform dimensionality reduction or feature selection techniques to mitigate the curse of dimensionality.\n",
    "Use cross-validation or grid search to choose an optimal value of K.\n",
    "Apply feature scaling (e.g., normalization, standardization) to ensure all features contribute equally to distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb5401",
   "metadata": {},
   "source": [
    "## Ans : 9\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space.\n",
    "It is calculated as the square root of the sum of the squared differences between corresponding coordinates.\n",
    "It measures the length of the shortest path between two points.\n",
    "Suitable for continuous features and when the relationship between features is well-represented by Euclidean space.\n",
    "Manhattan Distance:\n",
    "\n",
    "Manhattan distance (also known as taxicab or city block distance) is the sum of the absolute differences between corresponding coordinates of two points.\n",
    "It measures the distance a taxicab would have to travel in a city grid (only horizontal and vertical movements).\n",
    "Suitable when the relationship between features is better represented by a grid-like structure or when dealing with categorical features.\n",
    "In KNN, both Euclidean and Manhattan distances can be used as distance metrics to determine the nearest neighbors. The choice between them depends on the nature of the data and the problem at hand.\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is essential in KNN because the algorithm relies on distance calculations between data points. If the features have different scales or units, it can lead to misleading results. Feature scaling brings all the features to a similar scale, allowing them to contribute equally to the distance calculations.\n",
    "\n",
    "There are two common methods for feature scaling:\n",
    "\n",
    "Normalization (Min-Max Scaling): Scales the feature values to a range between 0 and 1 by subtracting the minimum value and dividing by the range (maximum - minimum).\n",
    "Standardization (Z-score Scaling): Transforms the feature values to have a mean of 0 and a standard deviation of 1 by subtracting the mean and dividing by the standard deviation.\n",
    "By applying feature scaling, all the features have comparable ranges, preventing any single feature from dominating the distance calculations and ensuring a fair comparison between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320462d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
